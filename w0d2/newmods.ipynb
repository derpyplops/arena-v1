{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch as t\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(nn.Module):\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        x[x < 0] = 0\n",
    "        return x\n",
    "\n",
    "utils.test_relu(ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self, start_dim: int = 1, end_dim: int = -1) -> None:\n",
    "        super().__init__()\n",
    "        self.start_dim = start_dim\n",
    "        self.end_dim = end_dim\n",
    "\n",
    "    def forward(self, input: t.Tensor) -> t.Tensor:\n",
    "        \"\"\"Flatten out dimensions from start_dim to end_dim, inclusive of both.\n",
    "        \"\"\"\n",
    "        shape = input.shape\n",
    "        \n",
    "        start_dim = self.start_dim\n",
    "        end_dim = self.end_dim if self.end_dim >= 0 else len(shape) + self.end_dim\n",
    "        \n",
    "        shape_left = shape[:start_dim]\n",
    "        shape_middle = math.prod(shape[start_dim:end_dim+1])\n",
    "        shape_right = shape[end_dim+1:]\n",
    "        \n",
    "        new_shape = shape_left + (shape_middle,) + shape_right\n",
    "        \n",
    "        return t.reshape(input, new_shape)\n",
    "\n",
    "utils.test_flatten(Flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Optional\n",
    "from fancy_einsum import einsum\n",
    "\n",
    "IntOrPair = Union[int, tuple[int, int]]\n",
    "Pair = tuple[int, int]\n",
    "\n",
    "def force_pair(v: IntOrPair) -> Pair:\n",
    "    \"\"\"Convert v to a pair of int, if it isn't already.\"\"\"\n",
    "    if isinstance(v, tuple):\n",
    "        if len(v) != 2:\n",
    "            raise ValueError(v)\n",
    "        return (int(v[0]), int(v[1]))\n",
    "    elif isinstance(v, int):\n",
    "        return (v, v)\n",
    "    raise ValueError(v)\n",
    "\n",
    "def pad1d(x: t.Tensor, left: int, right: int, pad_value: float) -> t.Tensor:\n",
    "    \"\"\"Return a new tensor with padding applied to the edges.\n",
    "\n",
    "    x: shape (batch, in_channels, width), dtype float32\n",
    "\n",
    "    Return: shape (batch, in_channels, left + right + width)\n",
    "    \"\"\"\n",
    "    B, C, W = x.shape\n",
    "    output = x.new_full(size=(B, C, left + W + right), fill_value=pad_value)\n",
    "    output[..., left : left + W] = x\n",
    "    # Note - you can't use `left:-right`, because `right` could be zero.\n",
    "    return output\n",
    "\n",
    "def pad2d(x: t.Tensor, left: int, right: int, top: int, bottom: int, pad_value: float) -> t.Tensor:\n",
    "    \"\"\"Return a new tensor with padding applied to the edges.\n",
    "\n",
    "    x: shape (batch, in_channels, height, width), dtype float32\n",
    "\n",
    "    Return: shape (batch, in_channels, top + height + bottom, left + width + right)\n",
    "    \"\"\"\n",
    "    B, C, H, W = x.shape\n",
    "    output = x.new_full(size=(B, C, top + H + bottom, left + W + right), fill_value=pad_value)\n",
    "    output[..., top : top + H, left : left + W] = x\n",
    "    return output\n",
    "\n",
    "def conv2d(x, weights, stride: IntOrPair = 1, padding: IntOrPair = 0) -> t.Tensor:\n",
    "    \"\"\"Like torch's conv2d using bias=False\n",
    "\n",
    "    x: shape (batch, in_channels, height, width)\n",
    "    weights: shape (out_channels, in_channels, kernel_height, kernel_width)\n",
    "\n",
    "\n",
    "    Returns: shape (batch, out_channels, output_height, output_width)\n",
    "    \"\"\"\n",
    "\n",
    "    stride_h, stride_w = force_pair(stride)\n",
    "    padding_h, padding_w = force_pair(padding)\n",
    "    \n",
    "    x_padded = pad2d(x, left=padding_w, right=padding_w, top=padding_h, bottom=padding_h, pad_value=0)\n",
    "    \n",
    "    batch, in_channels, height, width = x_padded.shape\n",
    "    out_channels, in_channels_2, kernel_height, kernel_width = weights.shape\n",
    "    assert in_channels == in_channels_2, \"in_channels for x and weights don't match up\"\n",
    "    output_width = 1 + (width - kernel_width) // stride_w\n",
    "    output_height = 1 + (height - kernel_height) // stride_h\n",
    "    \n",
    "    xsB, xsIC, xsH, xsW = x_padded.stride() # B for batch, IC for input channels, H for height, W for width\n",
    "    wsOC, wsIC, wsH, wsW = weights.stride()\n",
    "    \n",
    "    x_new_shape = (batch, in_channels, output_height, output_width, kernel_height, kernel_width)\n",
    "    x_new_stride = (xsB, xsIC, xsH * stride_h, xsW * stride_w, xsH, xsW)\n",
    "    \n",
    "    x_strided = x_padded.as_strided(size=x_new_shape, stride=x_new_stride)\n",
    "    \n",
    "    return einsum(\"B IC OH OW wH wW, OC IC wH wW -> B OC OH OW\", x_strided, weights)\n",
    "\n",
    "def maxpool2d(x: t.Tensor, kernel_size: IntOrPair, stride: Optional[IntOrPair] = None, padding: IntOrPair = 0\n",
    ") -> t.Tensor:\n",
    "    \"\"\"Like PyTorch's maxpool2d.\n",
    "\n",
    "    x: shape (batch, channels, height, width)\n",
    "    stride: if None, should be equal to the kernel size\n",
    "\n",
    "    Return: (batch, channels, output_height, output_width)\n",
    "    \"\"\"\n",
    "\n",
    "    if stride is None:\n",
    "        stride = kernel_size\n",
    "    stride_height, stride_width = force_pair(stride)\n",
    "    padding_height, padding_width = force_pair(padding)\n",
    "    kernel_height, kernel_width = force_pair(kernel_size)\n",
    "    \n",
    "    x_padded = pad2d(x, left=padding_width, right=padding_width, top=padding_height, bottom=padding_height, pad_value=-t.inf)\n",
    "    \n",
    "    batch, channels, height, width = x_padded.shape\n",
    "    output_width = 1 + (width - kernel_width) // stride_width\n",
    "    output_height = 1 + (height - kernel_height) // stride_height\n",
    "    \n",
    "    xsB, xsC, xsH, xsW = x_padded.stride()\n",
    "    \n",
    "    x_new_shape = (batch, channels, output_height, output_width, kernel_height, kernel_width)\n",
    "    x_new_stride = (xsB, xsC, xsH * stride_height, xsW * stride_width, xsH, xsW)\n",
    "    \n",
    "    x_strided = x_padded.as_strided(size=x_new_shape, stride=x_new_stride)\n",
    "    \n",
    "    output = t.amax(x_strided, dim=(-1, -2))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually verify that this is an informative repr: MaxPool2d()\n"
     ]
    }
   ],
   "source": [
    "class MaxPool2d(nn.Module):\n",
    "    def __init__(self, kernel_size: IntOrPair, stride: Optional[IntOrPair] = None, padding: IntOrPair = 1):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        '''Call the functional version of maxpool2d.'''\n",
    "        return maxpool2d(x, self.kernel_size, self.stride, self.padding)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        '''Add additional information to the string representation of this class.'''\n",
    "        pass\n",
    "\n",
    "utils.test_maxpool2d_module(MaxPool2d)\n",
    "m = MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "print(f\"Manually verify that this is an informative repr: {m}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import float16, float64, nn\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, bias=True):\n",
    "        '''A simple linear (technically, affine) transformation.\n",
    "\n",
    "        The fields should be named `weight` and `bias` for compatibility with PyTorch.\n",
    "        If `bias` is False, set `self.bias` to None.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.n_in = in_features\n",
    "        self.n_out = out_features\n",
    "        xavier_constant = 1 / math.sqrt(self.n_in)\n",
    "        self.weight = nn.Parameter(t.rand(out_features, in_features) * xavier_constant)\n",
    "        self.bias = nn.Parameter(t.rand(out_features,) * xavier_constant) if bias else None\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        '''\n",
    "        x: shape (*, in_features)\n",
    "        Return: shape (*, out_features)\n",
    "        '''\n",
    "        res = einsum(f'... n_in, n_out n_in -> ... n_out', x, self.weight)\n",
    "        if self.bias is not None:\n",
    "            res += self.bias\n",
    "\n",
    "        return res\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        pass\n",
    "\n",
    "\n",
    "utils.test_linear_forward(Linear)\n",
    "utils.test_linear_parameters(Linear)\n",
    "utils.test_linear_no_bias(Linear)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels: int, out_channels: int, kernel_size: IntOrPair, stride: IntOrPair = 1, padding: IntOrPair = 0\n",
    "    ):\n",
    "        '''\n",
    "        Same as torch.nn.Conv2d with bias=False.\n",
    "\n",
    "        Name your weight field `self.weight` for compatibility with the PyTorch version.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.n_in = in_channels\n",
    "        self.n_out = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        kernel_height, kernel_width = force_pair(kernel_size)\n",
    "\n",
    "        xavier_constant = 1 / math.sqrt(in_channels * kernel_width * kernel_height)\n",
    "        weight = xavier_constant * t.rand(out_channels, in_channels, kernel_height, kernel_width)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        '''Apply the functional conv2d you wrote earlier.'''\n",
    "        return conv2d(x, self.weight, self.stride, self.padding)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        pass\n",
    "\n",
    "utils.test_conv2d_module(Conv2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a41fdc720b403cff5d22ec3440153970555b5fcc336583b0458a17a41b31d53f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
