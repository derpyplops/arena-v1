{
 cells: [
  {
   cell_type: code
   execution_count: 23
   metadata: {}
   outputs: []
   source: [
    import torch as t
    import torch.nn as nn
    from einops import rearrange, repeat
    from fancy_einsum import einsum
    from torch import optim
    from torch.utils.data import DataLoader, Dataset, random_split
    from dataclasses import dataclass 
    from collections import OrderedDict
    import math
    from typing import Optional, Union
    import plotly.express as px
    import torchinfo
    import matplotlib as plt
    import os
    from tqdm.notebook import tqdm_notebook
    import transformers
    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
    import utils
    
    device = t.device('cuda' if t.cuda.is_available() else 'cpu')
    
    #@title Transformer Modules
    @dataclass(frozen=True)
    class TransformerConfig:
        '''Constants used throughout your decoder-only transformer model.'''
    
        num_layers: int
        num_heads: int
        vocab_size: int
        hidden_size: int # also embedding dim or d_model
        max_seq_len: int = 5000 
        dropout: float = 0.1
        layer_norm_epsilon: float = 1e-05
        device = t.device('cuda' if t.cuda.is_available() else 'cpu')
    
    bertconfig = TransformerConfig(
        num_layers = 12,
        num_heads = 12,
        vocab_size = 28996,
        hidden_size = 768,
        max_seq_len = 512,
        dropout = 0.1,
        layer_norm_epsilon = 1e-12
    )
    
    # config = TransformerConfig(
    #     num_layers = 12,
    #     num_heads = 12,
    #     vocab_size = 50257,
    #     hidden_size = 768,
    #     max_seq_len = 1024,
    #     dropout = 0.1,
    #     masking='autoregressive',
    #     layer_norm_epsilon = 1e-05
    # )
    
    class MultiLayerPerceptron(nn.Module):  
    
        def __init__(self, d_in: int, d_out: int):
            super().__init__()
            d_h = d_in * 4
            self.model = nn.Sequential(OrderedDict([
                ('linear1', nn.Linear(d_in, d_h)),
                ('GELU', nn.GELU()),
                ('linear2', nn.Linear(d_h, d_in)),   
                ('dropout', nn.Dropout(p=0.1))
            ]))
    
        def forward(self, x: t.Tensor):
            return self.model(x)
    
    class MultiheadMaskedAttention(nn.Module):
    
        def __init__(self, hidden_size: int, num_heads: int):
            super().__init__()
            self.W_QKV = nn.Linear(hidden_size, hidden_size * 3)
            self.W_O = nn.Linear(hidden_size, hidden_size)
            self.num_heads = num_heads
            self.dropout1 = nn.Dropout(p=0.1)
            self.dropout2 = nn.Dropout(p=0.1)
    
        def forward(self, x: t.Tensor, mask=None) -> t.Tensor:
            '''
            x: shape (batch, seq, hidden_size)
            Return: shape (batch, seq, hidden_size)
            '''
            Q, K, V = self.W_QKV(x).chunk(3, dim=-1)
            att = self.multihead_masked_attention(Q, K, V, self.num_heads)
            return self.W_O(att)
    
        def multihead_masked_attention(self, Q: t.Tensor, K: t.Tensor, V: t.Tensor, n_heads: int):
            '''
            Q: shape (b, s1, e)
            K: shape (b, s2, e)
            V: shape (b, s2, e)
    
            e = nheads * h
            b = batch
            s = seq_len
            h = hidden
    
            Return: shape (b s e)
            '''
    
            assert Q.shape[-1] % n_heads == 0
            assert K.shape[-1] % n_heads == 0
            assert V.shape[-1] % n_heads == 0
            assert K.shape[-1] == V.shape[-1]
    
            Q = rearrange(Q, 'b s (nheads h) -> b nheads s h', nheads=n_heads)
            K = rearrange(K, 'b s (nheads h) -> b nheads s h', nheads=n_heads)
            V = rearrange(V, 'b s (nheads h) -> b nheads s h', nheads=n_heads)
    
            batch, nheads, seq_len, headsize = Q.shape
    
            scaled_dot_prod = einsum('b nheads s1 h, b nheads s2 h -> b nheads s2 s1', K, Q) / (headsize ** 0.5)
            mask_filter = t.triu(t.full_like(scaled_dot_prod, -t.inf), 1)
            scaled_dot_prod += mask_filter
            attention_probs = scaled_dot_prod.softmax(dim=-1)
            attention_probs = self.dropout1(attention_probs)
            attention_vals = einsum('b nheads s1 s2, b nheads s2 c -> b nheads s1 c', attention_probs, V)
            attention = rearrange(attention_vals, 'b nheads s c -> b s (nheads c)')
            return self.dropout2(attention) 
    
    class GPT2DecoderBlock(nn.Module):
    
        def __init__(self, config: TransformerConfig):
            super().__init__()
            self.layernorm1 = nn.LayerNorm(config.hidden_size)
            self.attention = MultiheadMaskedAttention(
                hidden_size=config.hidden_size,
                num_heads=config.num_heads
            )
            self.layernorm2 = nn.LayerNorm(config.hidden_size)
            self.mlp = MultiLayerPerceptron(config.hidden_size, config.hidden_size)
        
        def forward(self, x: t.Tensor):
            x = x + self.attention(self.layernorm1(x))
            x = x + self.mlp(self.layernorm2(x))
            return x
    
    class GPT2(nn.Module):
    
        def __init__(self, config: TransformerConfig):
            super().__init__()
            self.embed = nn.Embedding(config.vocab_size, config.hidden_size)
            self.positional_encoding = nn.Embedding(config.max_seq_len, config.hidden_size)
            decoders = [GPT2DecoderBlock(config) for i in range(config.num_layers)]
            names = ['decoder' + str(i) for i in range(config.num_layers)]
            self.decoderlayer = nn.Sequential(OrderedDict(zip(names, decoders)))
            self.dropout = nn.Dropout(p=config.dropout)
            self.layernorm = nn.LayerNorm(config.hidden_size)
    
        def forward(self, tokens):
            if len(tokens.shape) == 1:
                tokens = rearrange(tokens, \seq -> 1 seq\)
            embedding = self.embed(tokens) # (b, seq_len) -> (b, seq_len, embedding)
            pos_enc = self.positional_encoding(t.arange(tokens.shape[1], device=tokens.device)) # (seq_len)
            a = self.dropout(embedding + pos_enc) # (b, seq_len, embedding)
            b = self.decoderlayer(a) # (b, seq_len, embedding)
            c = self.layernorm(b) @ self.embed.weight.T # (b, seq_len, embedding) @ (embedding, vocab_size) -> (b, seq_len, vocab_size)
            return c
    
    gpt2 = GPT2(config)
   ]
  }
  {
   cell_type: code
   execution_count: 24
   metadata: {}
   outputs: []
   source: [
    class MultiheadAttention(nn.Module):
    
        def __init__(self, config: TransformerConfig):
            super().__init__()
            hidden_size, num_heads = config.hidden_size, config.num_heads
            self.num_heads = num_heads
            self.W_Q = nn.Linear(hidden_size, hidden_size)
            self.W_K = nn.Linear(hidden_size, hidden_size)
            self.W_V = nn.Linear(hidden_size, hidden_size)
            self.W_O = nn.Linear(hidden_size, hidden_size)
            self.num_heads = num_heads
    
        def forward(self, x: t.Tensor, additive_attention_mask: Optional[t.Tensor] = None) -> t.Tensor:
            Q, K, V = self.W_Q(x), self.W_K(x), self.W_V(x)
            att = self.multihead_masked_attention(Q, K, V, self.num_heads, additive_attention_mask)
            return self.W_O(att)
    
        def multihead_masked_attention(self, Q: t.Tensor, K: t.Tensor, V: t.Tensor, n_heads: int, additive_attention_mask: Optional[t.Tensor]):
            '''
            Q: shape (b, s1, e)
            K: shape (b, s2, e)
            V: shape (b, s2, e)
    
            e = nheads * h
            b = batch
            s = seq_len
            h = hidden
    
            Return: shape (b s e)
            '''
    
            assert Q.shape[-1] % n_heads == 0
            assert K.shape[-1] % n_heads == 0
            assert V.shape[-1] % n_heads == 0
            assert K.shape[-1] == V.shape[-1]
    
            Q = rearrange(Q, 'b s (nheads h) -> b nheads s h', nheads=n_heads)
            K = rearrange(K, 'b s (nheads h) -> b nheads s h', nheads=n_heads)
            V = rearrange(V, 'b s (nheads h) -> b nheads s h', nheads=n_heads)
    
            batch, nheads, seq_len, headsize = Q.shape
    
            scaled_dot_prod = einsum('b nheads sk h, b nheads sq h -> b nheads sq sk', K, Q) / (headsize ** 0.5)
            scaled_dot_prod += additive_attention_mask # (batch, 1, 1, sk)
            attention_probs = scaled_dot_prod.softmax(dim=-1)
            attention_vals = einsum('b nheads s1 s2, b nheads s2 c -> b nheads s1 c', attention_probs, V)
            attention = rearrange(attention_vals, 'b nheads s c -> b s (nheads c)')
            return attention
    
    class BERTMLP(nn.Module):
        def __init__(self, config: TransformerConfig):
            super().__init__()
            hs, p_dropout = config.hidden_size, config.dropout
            self.mlp = nn.Sequential(OrderedDict([
                ('linear1', nn.Linear(hs, hs * 4)),
                ('GELU', nn.GELU()),
                ('linear2', nn.Linear(hs * 4, hs)),   
                ('dropout', nn.Dropout(p_dropout))
            ]))
    
        def forward(self, x: t.Tensor):
            return self.mlp(x)
    
    class BERTBlock(nn.Module):
    
        def __init__(self, config):
            super().__init__()
            self.attention = MultiheadAttention(config)
            self.ln1 = nn.LayerNorm(config.hidden_size)
            self.mlp = BERTMLP(config)
            self.ln2 = nn.LayerNorm(config.hidden_size)
            
        def forward(self, x: t.Tensor, additive_attention_mask: Optional[t.Tensor] = None) -> t.Tensor:
            '''
            x: shape (batch, seq, hidden_size)
            additive_attention_mask: shape (batch, nheads=1, seqQ=1, seqK)
            '''
            h1 = self.ln1(self.attention(x, additive_attention_mask)) # TODO chain this
            h2 = self.ln2(self.mlp(h1) + h1)
            return h2
    
    def make_additive_attention_mask(one_zero_attention_mask: t.Tensor, big_negative_number: float = -10000) -> t.Tensor:
        '''
        one_zero_attention_mask: 
            shape (batch, seq)
            Contains 1 if this is a valid token and 0 if it is a padding token.
    
        big_negative_number:
            Any negative number large enough in magnitude that exp(big_negative_number) is 0.0 for the floating point precision used.
    
        Out: 
            shape (batch, nheads=1, seqQ=1, seqK)
            Contains 0 if attention is allowed, big_negative_number if not.
        '''
        return rearrange((1 - one_zero_attention_mask) * big_negative_number, 'batch seq -> batch 1 1 seq')
    
    # util one is erroring
    def test_make_additive_attention_mask(make_additive_attention_mask):
        def make_additive_attention_mask_soln(one_zero_attention_mask: t.Tensor, big_negative_number: float = -10000) -> t.Tensor:
            '''
            one_zero_attention_mask: 
                shape (batch, seq)
                Contains 1 if this is a valid token and 0 if it is a padding token.
    
            big_negative_number:
                Any negative number large enough in magnitude that exp(big_negative_number) is 0.0 for the floating point precision used.
    
            Out: shape (batch, heads, seq, seq). Contains 0 if attention is allowed, and big_negative_number if it is not allowed.
            '''
            return big_negative_number * repeat(1 - one_zero_attention_mask, \batch seqK -> batch 1 1 seqK\)
        arr = t.randint(low=0, high=2, size=(3, 4))
        expected = make_additive_attention_mask_soln(arr)
        actual = make_additive_attention_mask(arr)
        t.testing.assert_close(expected, actual)
    
    class BertCommon(nn.Module):
    
        def __init__(self, config: TransformerConfig):
            super().__init__()
            self.token_emb = nn.Embedding(config.vocab_size, config.hidden_size)
            self.pos_emb = nn.Embedding(config.max_seq_len, config.hidden_size)
            self.tokentype_emb = nn.Embedding(2, config.hidden_size)
            self.ln1 = nn.LayerNorm(config.hidden_size)
            self.dropout1 = nn.Dropout(config.dropout)
            self.bertblocks = nn.ModuleList([BERTBlock(config) for i in range(config.num_layers)])
    
        def forward(
            self,
            input_ids: t.Tensor,
            one_zero_attention_mask: Optional[t.Tensor] = None,
            token_type_ids: Optional[t.Tensor] = None,
        ) -> t.Tensor:
            '''
            input_ids: (batch, seq) - the token ids
            one_zero_attention_mask: (batch, seq) - only used in training, passed to `make_additive_attention_mask` and used in the attention blocks.
            token_type_ids: (batch, seq) - only used for NSP, passed to token type embedding.
            '''
            token_embedding = self.token_emb(input_ids) # (b, seq_len, emb)
            batch, seq_len = input_ids.shape
            positional_embedding = self.pos_emb(t.arange(seq_len)) # (seq_len, emb)
            token_type_embedding = self.tokentype_emb(token_type_ids) # (b, seq_len, emb)
            x = self.dropout1(self.ln1(token_embedding + positional_embedding + token_type_embedding))
            mask = make_additive_attention_mask(one_zero_attention_mask) if one_zero_attention_mask else None
            for block in self.bertblocks:
                x = block(x, mask)
            return x
    
    class BertLanguageModel(nn.Module):
    
        def __init__(self, config: TransformerConfig):
            super().__init__()
            hs = config.hidden_size
            self.bertcommon = BertCommon(config)
            self.linear = nn.Linear(hs, hs)
            self.gelu = nn.GELU()
            self.ln = nn.LayerNorm(config.hidden_size)
            xavier = 1 / (config.vocab_size ** 0.5)
            self.unembed_bias = nn.parameter.Parameter(t.randn(config.vocab_size) * 2 * xavier - xavier) # N(-xavier, xavier)
    
        def forward(
            self,
            input_ids: t.Tensor,
            one_zero_attention_mask: Optional[t.Tensor] = None,
            token_type_ids: Optional[t.Tensor] = None,
        ) -> t.Tensor:
            '''
            input_ids: (batch, seq) - the token ids
            one_zero_attention_mask: (batch, seq) - only used in training, passed to `make_additive_attention_mask` and used in the attention blocks.
            token_type_ids: (batch, seq) - only used for NSP, passed to token type embedding.
            '''
            unembed = self.bertcommon.token_emb.weight.T
            x = self.bertcommon(input_ids, one_zero_attention_mask, token_type_ids)
            x = self.linear(x)
            x = self.gelu(x)
            x = self.ln(x)
            return x @ unembed + self.unembed_bias
    
    test_make_additive_attention_mask(make_additive_attention_mask)
   ]
  }
  {
   cell_type: code
   execution_count: 25
   metadata: {}
   outputs: []
   source: [
    my_bert = BertLanguageModel(bertconfig)
    bert = transformers.BertForMaskedLM.from_pretrained(\bert-base-cased\)
    utils.print_param_count(my_bert, bert)
   ]
  }
    {
     ename: KeyboardInterrupt
     evalue: 
     output_type: error
     traceback: [
      \u001b[0;31m---------------------------------------------------------------------------\u001b[0m
      \u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)
      Cell \u001b[0;32mIn [22], line 38\u001b[0m\u001b[1;32m     34\u001b[0m     my_gpt\u001b[39m.\u001b[39mload_state_dict(state_dict)\u001b[1;32m     36\u001b[0m     \u001b[39mreturn\u001b[39;00m my_gpt\u001b[0;32m---> 38\u001b[0m my_gpt \u001b[39m=\u001b[39m copy_weights(my_bert, bert)
      Cell \u001b[0;32mIn [22], line 19\u001b[0m, in \u001b[0;36mcopy_weights\u001b[0;34m(my_bert, bert)\u001b[0m\u001b[1;32m     16\u001b[0m idx \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\u001b[1;32m     18\u001b[0m \u001b[39mwhile\u001b[39;00m my_idx \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(my_bert_dict):\u001b[0;32m---> 19\u001b[0m     my_param_name, my_param \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(my_bert_dict\u001b[39m.\u001b[39;49mitems())[my_idx]\u001b[1;32m     20\u001b[0m     name, param \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(bert_dict\u001b[39m.\u001b[39mitems())[idx]\u001b[1;32m     21\u001b[0m     \u001b[39m# Sometimes params are transposed\u001b[39;00m
      \u001b[0;31mKeyboardInterrupt\u001b[0m: 
     ]
    }
   ]
   source: [
    def copy_weights(my_bert: BertLanguageModel, bert) -> BertLanguageModel:
        '''Copy over the weights of `gpt` to your gpt implementation.'''
    
        # Here we use named params not state dict, because gpt doesn't have any buffers we care about
        # (I think all its buffers are attention masks)
        my_bert_dict = dict(my_bert.named_parameters())
        bert_dict = dict(bert.named_parameters())
        
        # Check the number of params/buffers is correct
        assert len(my_bert_dict) == len(bert_dict), umber of layers is wrong. Have you done the prev step correctly?\
        
        # Initialise an empty dictionary to store the correct key-value pairs
        state_dict = {}
    
        my_idx = 1
        idx = 0
    
        while my_idx < len(my_bert_dict):
            my_param_name, my_param = list(my_bert_dict.items())[my_idx]
            name, param = list(bert_dict.items())[idx]
            # Sometimes params are transposed
            if my_param.shape == param.shape:
                state_dict[my_param_name] = param
                print(f\Copied params.T: {name} -> {my_param_name}\)
            else:
                raise Exception('Wrong param')
                
            # else:
                # raise Exception(f\Parameter shapes don't match: {my_param.shape} vs {param.shape}\)
    
        # if set(state_dict.keys()) != set(my_gpt.state_dict().keys()):
        #     raise Exception(\State dicts don't match.\)
        
        my_gpt.load_state_dict(state_dict)
        
        return my_gpt
    
    my_gpt = copy_weights(my_bert, bert)
   ]
  }
 ]
 metadata: {
  kernelspec: {
   display_name: Python 3.9.6 ('venv': venv)
   language: python
   name: python3
  }
  language_info: {
   codemirror_mode: {
    name: ipython
    version: 3
   }
   file_extension: .py
   mimetype: text/x-python
   name: python
   nbconvert_exporter: python
   pygments_lexer: ipython3
   version: 3.9.6
  }
  orig_nbformat: 4
  vscode: {
   interpreter: {
    hash: a41fdc720b403cff5d22ec3440153970555b5fcc336583b0458a17a41b31d53f
   }
  }
 }
 nbformat: 4
 nbformat_minor: 2
}
