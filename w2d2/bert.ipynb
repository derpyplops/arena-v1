{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, repeat\n",
    "from fancy_einsum import einsum\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from dataclasses import dataclass \n",
    "from collections import OrderedDict\n",
    "import math\n",
    "from typing import Optional, Union\n",
    "import plotly.express as px\n",
    "import torchinfo\n",
    "import matplotlib as plt\n",
    "import os\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import transformers\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import utils\n",
    "\n",
    "device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "\n",
    "#@title Transformer Modules\n",
    "@dataclass(frozen=True)\n",
    "class TransformerConfig:\n",
    "    '''Constants used throughout your decoder-only transformer model.'''\n",
    "\n",
    "    num_layers: int\n",
    "    num_heads: int\n",
    "    vocab_size: int\n",
    "    hidden_size: int # also embedding dim or d_model\n",
    "    max_seq_len: int = 5000 \n",
    "    dropout: float = 0.1\n",
    "    layer_norm_epsilon: float = 1e-05\n",
    "    device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "\n",
    "bertconfig = TransformerConfig(\n",
    "    num_layers = 12,\n",
    "    num_heads = 12,\n",
    "    vocab_size = 28996,\n",
    "    hidden_size = 768,\n",
    "    max_seq_len = 512,\n",
    "    dropout = 0.1,\n",
    "    layer_norm_epsilon = 1e-12\n",
    ")\n",
    "\n",
    "# config = TransformerConfig(\n",
    "#     num_layers = 12,\n",
    "#     num_heads = 12,\n",
    "#     vocab_size = 50257,\n",
    "#     hidden_size = 768,\n",
    "#     max_seq_len = 1024,\n",
    "#     dropout = 0.1,\n",
    "#     masking='autoregressive',\n",
    "#     layer_norm_epsilon = 1e-05\n",
    "# )\n",
    "\n",
    "class MultiLayerPerceptron(nn.Module):  \n",
    "\n",
    "    def __init__(self, d_in: int, d_out: int):\n",
    "        super().__init__()\n",
    "        d_h = d_in * 4\n",
    "        self.model = nn.Sequential(OrderedDict([\n",
    "            ('linear1', nn.Linear(d_in, d_h)),\n",
    "            ('GELU', nn.GELU()),\n",
    "            ('linear2', nn.Linear(d_h, d_in)),   \n",
    "            ('dropout', nn.Dropout(p=0.1))\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x: t.Tensor):\n",
    "        return self.model(x)\n",
    "\n",
    "class MultiheadMaskedAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.W_QKV = nn.Linear(hidden_size, hidden_size * 3)\n",
    "        self.W_O = nn.Linear(hidden_size, hidden_size)\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout1 = nn.Dropout(p=0.1)\n",
    "        self.dropout2 = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, x: t.Tensor, mask=None) -> t.Tensor:\n",
    "        '''\n",
    "        x: shape (batch, seq, hidden_size)\n",
    "        Return: shape (batch, seq, hidden_size)\n",
    "        '''\n",
    "        Q, K, V = self.W_QKV(x).chunk(3, dim=-1)\n",
    "        att = self.multihead_masked_attention(Q, K, V, self.num_heads)\n",
    "        return self.W_O(att)\n",
    "\n",
    "    def multihead_masked_attention(self, Q: t.Tensor, K: t.Tensor, V: t.Tensor, n_heads: int):\n",
    "        '''\n",
    "        Q: shape (b, s1, e)\n",
    "        K: shape (b, s2, e)\n",
    "        V: shape (b, s2, e)\n",
    "\n",
    "        e = nheads * h\n",
    "        b = batch\n",
    "        s = seq_len\n",
    "        h = hidden\n",
    "\n",
    "        Return: shape (b s e)\n",
    "        '''\n",
    "\n",
    "        assert Q.shape[-1] % n_heads == 0\n",
    "        assert K.shape[-1] % n_heads == 0\n",
    "        assert V.shape[-1] % n_heads == 0\n",
    "        assert K.shape[-1] == V.shape[-1]\n",
    "\n",
    "        Q = rearrange(Q, 'b s (nheads h) -> b nheads s h', nheads=n_heads)\n",
    "        K = rearrange(K, 'b s (nheads h) -> b nheads s h', nheads=n_heads)\n",
    "        V = rearrange(V, 'b s (nheads h) -> b nheads s h', nheads=n_heads)\n",
    "\n",
    "        batch, nheads, seq_len, headsize = Q.shape\n",
    "\n",
    "        scaled_dot_prod = einsum('b nheads s1 h, b nheads s2 h -> b nheads s2 s1', K, Q) / (headsize ** 0.5)\n",
    "        mask_filter = t.triu(t.full_like(scaled_dot_prod, -t.inf), 1)\n",
    "        scaled_dot_prod += mask_filter\n",
    "        attention_probs = scaled_dot_prod.softmax(dim=-1)\n",
    "        attention_probs = self.dropout1(attention_probs)\n",
    "        attention_vals = einsum('b nheads s1 s2, b nheads s2 c -> b nheads s1 c', attention_probs, V)\n",
    "        attention = rearrange(attention_vals, 'b nheads s c -> b s (nheads c)')\n",
    "        return self.dropout2(attention) \n",
    "\n",
    "class GPT2DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.layernorm1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.attention = MultiheadMaskedAttention(\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_heads=config.num_heads\n",
    "        )\n",
    "        self.layernorm2 = nn.LayerNorm(config.hidden_size)\n",
    "        self.mlp = MultiLayerPerceptron(config.hidden_size, config.hidden_size)\n",
    "    \n",
    "    def forward(self, x: t.Tensor):\n",
    "        x = x + self.attention(self.layernorm1(x))\n",
    "        x = x + self.mlp(self.layernorm2(x))\n",
    "        return x\n",
    "\n",
    "class GPT2(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.positional_encoding = nn.Embedding(config.max_seq_len, config.hidden_size)\n",
    "        decoders = [GPT2DecoderBlock(config) for i in range(config.num_layers)]\n",
    "        names = ['decoder' + str(i) for i in range(config.num_layers)]\n",
    "        self.decoderlayer = nn.Sequential(OrderedDict(zip(names, decoders)))\n",
    "        self.dropout = nn.Dropout(p=config.dropout)\n",
    "        self.layernorm = nn.LayerNorm(config.hidden_size)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        if len(tokens.shape) == 1:\n",
    "            tokens = rearrange(tokens, \"seq -> 1 seq\")\n",
    "        embedding = self.embed(tokens) # (b, seq_len) -> (b, seq_len, embedding)\n",
    "        pos_enc = self.positional_encoding(t.arange(tokens.shape[1], device=tokens.device)) # (seq_len)\n",
    "        a = self.dropout(embedding + pos_enc) # (b, seq_len, embedding)\n",
    "        b = self.decoderlayer(a) # (b, seq_len, embedding)\n",
    "        c = self.layernorm(b) @ self.embed.weight.T # (b, seq_len, embedding) @ (embedding, vocab_size) -> (b, seq_len, vocab_size)\n",
    "        return c\n",
    "\n",
    "gpt2 = GPT2(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        hidden_size, num_heads = config.hidden_size, config.num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.W_Q = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_K = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_V = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_O = nn.Linear(hidden_size, hidden_size)\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def forward(self, x: t.Tensor, additive_attention_mask: Optional[t.Tensor] = None) -> t.Tensor:\n",
    "        Q, K, V = self.W_Q(x), self.W_K(x), self.W_V(x)\n",
    "        att = self.multihead_masked_attention(Q, K, V, self.num_heads, additive_attention_mask)\n",
    "        return self.W_O(att)\n",
    "\n",
    "    def multihead_masked_attention(self, Q: t.Tensor, K: t.Tensor, V: t.Tensor, n_heads: int, additive_attention_mask: Optional[t.Tensor]):\n",
    "        '''\n",
    "        Q: shape (b, s1, e)\n",
    "        K: shape (b, s2, e)\n",
    "        V: shape (b, s2, e)\n",
    "\n",
    "        e = nheads * h\n",
    "        b = batch\n",
    "        s = seq_len\n",
    "        h = hidden\n",
    "\n",
    "        Return: shape (b s e)\n",
    "        '''\n",
    "\n",
    "        assert Q.shape[-1] % n_heads == 0\n",
    "        assert K.shape[-1] % n_heads == 0\n",
    "        assert V.shape[-1] % n_heads == 0\n",
    "        assert K.shape[-1] == V.shape[-1]\n",
    "\n",
    "        Q = rearrange(Q, 'b s (nheads h) -> b nheads s h', nheads=n_heads)\n",
    "        K = rearrange(K, 'b s (nheads h) -> b nheads s h', nheads=n_heads)\n",
    "        V = rearrange(V, 'b s (nheads h) -> b nheads s h', nheads=n_heads)\n",
    "\n",
    "        batch, nheads, seq_len, headsize = Q.shape\n",
    "\n",
    "        scaled_dot_prod = einsum('b nheads sk h, b nheads sq h -> b nheads sq sk', K, Q) / (headsize ** 0.5)\n",
    "        scaled_dot_prod += additive_attention_mask # (batch, 1, 1, sk)\n",
    "        attention_probs = scaled_dot_prod.softmax(dim=-1)\n",
    "        attention_vals = einsum('b nheads s1 s2, b nheads s2 c -> b nheads s1 c', attention_probs, V)\n",
    "        attention = rearrange(attention_vals, 'b nheads s c -> b s (nheads c)')\n",
    "        return attention\n",
    "\n",
    "class BERTMLP(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        hs, p_dropout = config.hidden_size, config.dropout\n",
    "        self.mlp = nn.Sequential(OrderedDict([\n",
    "            ('linear1', nn.Linear(hs, hs * 4)),\n",
    "            ('GELU', nn.GELU()),\n",
    "            ('linear2', nn.Linear(hs * 4, hs)),   \n",
    "            ('dropout', nn.Dropout(p_dropout))\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x: t.Tensor):\n",
    "        return self.mlp(x)\n",
    "\n",
    "class BERTBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = MultiheadAttention(config)\n",
    "        self.ln1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.mlp = BERTMLP(config)\n",
    "        self.ln2 = nn.LayerNorm(config.hidden_size)\n",
    "        \n",
    "    def forward(self, x: t.Tensor, additive_attention_mask: Optional[t.Tensor] = None) -> t.Tensor:\n",
    "        '''\n",
    "        x: shape (batch, seq, hidden_size)\n",
    "        additive_attention_mask: shape (batch, nheads=1, seqQ=1, seqK)\n",
    "        '''\n",
    "        h1 = self.ln1(self.attention(x, additive_attention_mask)) # TODO chain this\n",
    "        h2 = self.ln2(self.mlp(h1) + h1)\n",
    "        return h2\n",
    "\n",
    "def make_additive_attention_mask(one_zero_attention_mask: t.Tensor, big_negative_number: float = -10000) -> t.Tensor:\n",
    "    '''\n",
    "    one_zero_attention_mask: \n",
    "        shape (batch, seq)\n",
    "        Contains 1 if this is a valid token and 0 if it is a padding token.\n",
    "\n",
    "    big_negative_number:\n",
    "        Any negative number large enough in magnitude that exp(big_negative_number) is 0.0 for the floating point precision used.\n",
    "\n",
    "    Out: \n",
    "        shape (batch, nheads=1, seqQ=1, seqK)\n",
    "        Contains 0 if attention is allowed, big_negative_number if not.\n",
    "    '''\n",
    "    return rearrange((1 - one_zero_attention_mask) * big_negative_number, 'batch seq -> batch 1 1 seq')\n",
    "\n",
    "# util one is erroring\n",
    "def test_make_additive_attention_mask(make_additive_attention_mask):\n",
    "    def make_additive_attention_mask_soln(one_zero_attention_mask: t.Tensor, big_negative_number: float = -10000) -> t.Tensor:\n",
    "        '''\n",
    "        one_zero_attention_mask: \n",
    "            shape (batch, seq)\n",
    "            Contains 1 if this is a valid token and 0 if it is a padding token.\n",
    "\n",
    "        big_negative_number:\n",
    "            Any negative number large enough in magnitude that exp(big_negative_number) is 0.0 for the floating point precision used.\n",
    "\n",
    "        Out: shape (batch, heads, seq, seq). Contains 0 if attention is allowed, and big_negative_number if it is not allowed.\n",
    "        '''\n",
    "        return big_negative_number * repeat(1 - one_zero_attention_mask, \"batch seqK -> batch 1 1 seqK\")\n",
    "    arr = t.randint(low=0, high=2, size=(3, 4))\n",
    "    expected = make_additive_attention_mask_soln(arr)\n",
    "    actual = make_additive_attention_mask(arr)\n",
    "    t.testing.assert_close(expected, actual)\n",
    "\n",
    "class BertCommon(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.pos_emb = nn.Embedding(config.max_seq_len, config.hidden_size)\n",
    "        self.tokentype_emb = nn.Embedding(2, config.hidden_size)\n",
    "        self.ln1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.dropout1 = nn.Dropout(config.dropout)\n",
    "        self.bertblocks = nn.ModuleList([BERTBlock(config) for i in range(config.num_layers)])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: t.Tensor,\n",
    "        one_zero_attention_mask: Optional[t.Tensor] = None,\n",
    "        token_type_ids: Optional[t.Tensor] = None,\n",
    "    ) -> t.Tensor:\n",
    "        '''\n",
    "        input_ids: (batch, seq) - the token ids\n",
    "        one_zero_attention_mask: (batch, seq) - only used in training, passed to `make_additive_attention_mask` and used in the attention blocks.\n",
    "        token_type_ids: (batch, seq) - only used for NSP, passed to token type embedding.\n",
    "        '''\n",
    "        token_embedding = self.token_emb(input_ids) # (b, seq_len, emb)\n",
    "        batch, seq_len = input_ids.shape\n",
    "        positional_embedding = self.pos_emb(t.arange(seq_len)) # (seq_len, emb)\n",
    "        token_type_embedding = self.tokentype_emb(token_type_ids) # (b, seq_len, emb)\n",
    "        x = self.dropout1(self.ln1(token_embedding + positional_embedding + token_type_embedding))\n",
    "        mask = make_additive_attention_mask(one_zero_attention_mask) if one_zero_attention_mask else None\n",
    "        for block in self.bertblocks:\n",
    "            x = block(x, mask)\n",
    "        return x\n",
    "\n",
    "class BertLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        hs = config.hidden_size\n",
    "        self.bertcommon = BertCommon(config)\n",
    "        self.linear = nn.Linear(hs, hs)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.ln = nn.LayerNorm(config.hidden_size)\n",
    "        xavier = 1 / (config.vocab_size ** 0.5)\n",
    "        self.unembed_bias = nn.parameter.Parameter(t.randn(config.vocab_size) * 2 * xavier - xavier) # N(-xavier, xavier)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: t.Tensor,\n",
    "        one_zero_attention_mask: Optional[t.Tensor] = None,\n",
    "        token_type_ids: Optional[t.Tensor] = None,\n",
    "    ) -> t.Tensor:\n",
    "        '''\n",
    "        input_ids: (batch, seq) - the token ids\n",
    "        one_zero_attention_mask: (batch, seq) - only used in training, passed to `make_additive_attention_mask` and used in the attention blocks.\n",
    "        token_type_ids: (batch, seq) - only used for NSP, passed to token type embedding.\n",
    "        '''\n",
    "        unembed = self.bertcommon.token_emb.weight.T\n",
    "        x = self.bertcommon(input_ids, one_zero_attention_mask, token_type_ids)\n",
    "        x = self.linear(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.ln(x)\n",
    "        return x @ unembed + self.unembed_bias\n",
    "\n",
    "test_make_additive_attention_mask(make_additive_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bert = BertLanguageModel(bertconfig)\n",
    "bert = transformers.BertForMaskedLM.from_pretrained(\"bert-base-cased\")\n",
    "utils.print_param_count(my_bert, bert)"
   ]
  },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [22], line 38\u001b[0m\n\u001b[1;32m     34\u001b[0m     my_gpt\u001b[39m.\u001b[39mload_state_dict(state_dict)\n\u001b[1;32m     36\u001b[0m     \u001b[39mreturn\u001b[39;00m my_gpt\n\u001b[0;32m---> 38\u001b[0m my_gpt \u001b[39m=\u001b[39m copy_weights(my_bert, bert)\n",
      "Cell \u001b[0;32mIn [22], line 19\u001b[0m, in \u001b[0;36mcopy_weights\u001b[0;34m(my_bert, bert)\u001b[0m\n\u001b[1;32m     16\u001b[0m idx \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[39mwhile\u001b[39;00m my_idx \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(my_bert_dict):\n\u001b[0;32m---> 19\u001b[0m     my_param_name, my_param \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(my_bert_dict\u001b[39m.\u001b[39;49mitems())[my_idx]\n\u001b[1;32m     20\u001b[0m     name, param \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(bert_dict\u001b[39m.\u001b[39mitems())[idx]\n\u001b[1;32m     21\u001b[0m     \u001b[39m# Sometimes params are transposed\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def copy_weights(my_bert: BertLanguageModel, bert) -> BertLanguageModel:\n",
    "    '''Copy over the weights of `gpt` to your gpt implementation.'''\n",
    "\n",
    "    # Here we use named params not state dict, because gpt doesn't have any buffers we care about\n",
    "    # (I think all its buffers are attention masks)\n",
    "    my_bert_dict = dict(my_bert.named_parameters())\n",
    "    bert_dict = dict(bert.named_parameters())\n",
    "    \n",
    "    # Check the number of params/buffers is correct\n",
    "    assert len(my_bert_dict) == len(bert_dict), \"Number of layers is wrong. Have you done the prev step correctly?\"\n",
    "    \n",
    "    # Initialise an empty dictionary to store the correct key-value pairs\n",
    "    state_dict = {}\n",
    "\n",
    "    my_idx = 1\n",
    "    idx = 0\n",
    "\n",
    "    while my_idx < len(my_bert_dict):\n",
    "        my_param_name, my_param = list(my_bert_dict.items())[my_idx]\n",
    "        name, param = list(bert_dict.items())[idx]\n",
    "        # Sometimes params are transposed\n",
    "        if my_param.shape == param.shape:\n",
    "            state_dict[my_param_name] = param\n",
    "            print(f\"Copied params.T: {name} -> {my_param_name}\")\n",
    "        else:\n",
    "            raise Exception('Wrong param')\n",
    "            \n",
    "        # else:\n",
    "            # raise Exception(f\"Parameter shapes don't match: {my_param.shape} vs {param.shape}\")\n",
    "\n",
    "    # if set(state_dict.keys()) != set(my_gpt.state_dict().keys()):\n",
    "    #     raise Exception(\"State dicts don't match.\")\n",
    "    \n",
    "    my_gpt.load_state_dict(state_dict)\n",
    "    \n",
    "    return my_gpt\n",
    "\n",
    "my_gpt = copy_weights(my_bert, bert)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a41fdc720b403cff5d22ec3440153970555b5fcc336583b0458a17a41b31d53f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
