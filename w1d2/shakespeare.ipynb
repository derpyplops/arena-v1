{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # download the complete works of shakespeare as a text file and save it in the home directory\n",
    "# !wget https://www.gutenberg.org/files/100/100-0.txt -O ./shakespeare.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/derpyplops/arena/blob/main/shakespeare.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, repeat\n",
    "from fancy_einsum import einsum\n",
    "from torch import optim\n",
    "from impl.transformer_modules import DecoderTransformer, TransformerConfig\n",
    "from torch.utils.data import DataLoader, Dataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeff', 'The', ' ', 'Project', ' ', 'Gutenberg', ' ', 'eBook', ' ', 'of', ' ', 'The', ' ', 'Complete', ' ', 'Works', ' ', 'of', ' ', 'William', ' ', 'Shakespeare', ', ', 'by', ' ', 'William', ' ', 'Shakespeare', '\\n\\n', 'This', ' ', 'eBook', ' ', 'is', ' ', 'for', ' ', 'the', ' ', 'use', ' ', 'of', ' ', 'anyone', ' ', 'anywhere', ' ', 'in', ' ', 'the', ' ', 'United', ' ', 'States', ' ', 'and', '\\n', 'most', ' ', 'other', ' ', 'parts', ' ', 'of', ' ', 'the', ' ', 'world', ' ', 'at', ' ', 'no', ' ', 'cost', ' ', 'and', ' ', 'with', ' ', 'almost', ' ', 'no', ' ', 'restrictions', '\\n', 'whatsoever', '. ', 'You', ' ', 'may', ' ', 'copy', ' ', 'it', ', ', 'give', ' ', 'it', ' ', 'away']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# read the file\n",
    "with open('./shakespeare.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(re.split(r\"\\b\", text)[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset containing shakespeare\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import re\n",
    "\n",
    "# read the file\n",
    "with open('./shakespeare.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, text, seq_size):\n",
    "        super().__init__()\n",
    "        self.text = text\n",
    "        self.vocab = sorted(set(text))\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.char_to_idx = {c: i for i, c in enumerate(self.vocab)}\n",
    "        self.idx_to_char = {i: c for i, c in enumerate(self.vocab)}\n",
    "        self.text_as_int = t.tensor([self.char_to_idx[c] for c in self.text])\n",
    "\n",
    "        self.seq_size = seq_size\n",
    "\n",
    "        self.num_batches = int(len(text) / (seq_size))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text_as_int[idx * self.seq_size:(idx + 1) * self.seq_size]\n",
    "        label = self.text_as_int[idx * self.seq_size + 1:(idx + 1) * self.seq_size + 1]\n",
    "        return (text, label)\n",
    "\n",
    "    def to_text(self, idxs):\n",
    "        return ''.join([self.idx_to_char[idx] for idx in idxs])\n",
    "\n",
    "    def to_int(self, text):\n",
    "        return [self.char_to_idx[c] for c in text]\n",
    "\n",
    "    def to_one_hot(self, idxs):\n",
    "        return t.eye(self.vocab_size)[idxs]\n",
    "\n",
    "    def to_text_from_one_hot(self, one_hot):\n",
    "        return self.to_text(t.argmax(one_hot, dim=-1))\n",
    "\n",
    "# create the dataset\n",
    "shakespeare_dataset = ShakespeareDataset(re.split(r\"\\b\", text), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([34542,  9992,   113,  8237,   113,  5523,   113, 17830,   113, 24979,\n",
      "          113,  9992,   113,  3477,   113, 10995,   113, 24979,   113, 10916,\n",
      "          113,  9165,   480, 14228,   113, 10916,   113,  9165,     1, 10039,\n",
      "          113, 17830,   113, 22293,   113, 19582,   113, 31392,   113, 33037,\n",
      "          113, 24979,   113, 12315,   113, 12317,   113, 21768,   113, 31392,\n",
      "          113, 10403,   113,  9566,   113, 12244,     0, 24317,   113, 25174,\n",
      "          113, 25577,   113, 24979,   113, 31392,   113, 34221,   113, 12640,\n",
      "          113, 24717,   113, 15883,   113, 12244,   113, 34099,   113, 12140,\n",
      "          113, 24717,   113, 28036,     0, 33837,   786, 11076,   113, 23721,\n",
      "          113, 15820,   113, 22310,   480, 20265,   113, 22310,   113, 12779])\n",
      "tensor([ 9992,   113,  8237,   113,  5523,   113, 17830,   113, 24979,   113,\n",
      "         9992,   113,  3477,   113, 10995,   113, 24979,   113, 10916,   113,\n",
      "         9165,   480, 14228,   113, 10916,   113,  9165,     1, 10039,   113,\n",
      "        17830,   113, 22293,   113, 19582,   113, 31392,   113, 33037,   113,\n",
      "        24979,   113, 12315,   113, 12317,   113, 21768,   113, 31392,   113,\n",
      "        10403,   113,  9566,   113, 12244,     0, 24317,   113, 25174,   113,\n",
      "        25577,   113, 24979,   113, 31392,   113, 34221,   113, 12640,   113,\n",
      "        24717,   113, 15883,   113, 12244,   113, 34099,   113, 12140,   113,\n",
      "        24717,   113, 28036,     0, 33837,   786, 11076,   113, 23721,   113,\n",
      "        15820,   113, 22310,   480, 20265,   113, 22310,   113, 12779,   113])\n"
     ]
    }
   ],
   "source": [
    "# print(shakespeare_dataset.text[0:52])\n",
    "# print(shakespeare_dataset.vocab)\n",
    "\n",
    "for x, y in shakespeare_dataset:\n",
    "    print(x)\n",
    "    print(y)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config: TransformerConfig, model, train_dataloader: DataLoader, optimizer, criterion):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model = DecoderTransformer(config)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    accuracy_list = []\n",
    "\n",
    "    for epoch in range(3):  # loop over the dataset multiple times\n",
    "        accuracy = 0\n",
    "        total = 0\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_dataloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(\n",
    "                rearrange(outputs, 'batch seq vocab -> batch vocab seq'),\n",
    "                labels\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            print(i)\n",
    "            # if i % 20 == 19:\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss:.5f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "            \n",
    "            \n",
    "        # for (x, y) in train_dataloader:\n",
    "\n",
    "        #     x = x.to(config.device)\n",
    "        #     y = y.to(config.device)\n",
    "\n",
    "        #     y_hat = model(x)\n",
    "        #     y_predictions = y_hat.argmax(2)\n",
    "        #     accuracy += (y_predictions == y).sum().item()\n",
    "        #     total += y.size(0) * 6\n",
    "\n",
    "        #     accuracy_list.append(accuracy/total)\n",
    "        # print(f'accuracy: {accuracy/total:.3f}')\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "    return accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[1,     1] loss: 199.93967\n",
      "1\n",
      "[1,     2] loss: 191.31787\n",
      "2\n",
      "[1,     3] loss: 181.74298\n",
      "3\n",
      "[1,     4] loss: 174.21786\n",
      "4\n",
      "[1,     5] loss: 164.65164\n",
      "5\n",
      "[1,     6] loss: 153.55565\n",
      "6\n",
      "[1,     7] loss: 143.33014\n",
      "7\n",
      "[1,     8] loss: 132.33330\n",
      "8\n",
      "[1,     9] loss: 123.01982\n",
      "9\n",
      "[1,    10] loss: 112.78036\n",
      "10\n",
      "[1,    11] loss: 102.35070\n",
      "11\n",
      "[1,    12] loss: 92.07424\n",
      "12\n",
      "[1,    13] loss: 82.79670\n",
      "13\n",
      "[1,    14] loss: 74.98795\n",
      "14\n",
      "[1,    15] loss: 67.26454\n",
      "15\n",
      "[1,    16] loss: 59.73179\n",
      "16\n",
      "[1,    17] loss: 55.01676\n",
      "17\n",
      "[1,    18] loss: 52.07555\n",
      "18\n",
      "[1,    19] loss: 49.65736\n",
      "19\n",
      "[1,    20] loss: 48.31279\n",
      "20\n",
      "[1,    21] loss: 46.10990\n",
      "21\n",
      "[1,    22] loss: 44.47622\n",
      "22\n",
      "[1,    23] loss: 43.51808\n",
      "23\n",
      "[1,    24] loss: 42.71238\n",
      "24\n",
      "[1,    25] loss: 41.77654\n",
      "25\n",
      "[1,    26] loss: 41.26542\n",
      "26\n",
      "[1,    27] loss: 40.75078\n",
      "27\n",
      "[1,    28] loss: 39.27156\n",
      "28\n",
      "[1,    29] loss: 39.28583\n",
      "29\n",
      "[1,    30] loss: 38.65879\n",
      "30\n",
      "[1,    31] loss: 37.96714\n",
      "31\n",
      "[1,    32] loss: 38.37193\n",
      "32\n",
      "[1,    33] loss: 36.63941\n",
      "33\n",
      "[1,    34] loss: 37.01942\n",
      "34\n",
      "[1,    35] loss: 36.12509\n",
      "35\n",
      "[1,    36] loss: 35.41640\n",
      "36\n",
      "[1,    37] loss: 35.28877\n",
      "37\n",
      "[1,    38] loss: 36.20525\n",
      "38\n",
      "[1,    39] loss: 35.70317\n",
      "39\n",
      "[1,    40] loss: 34.61444\n",
      "40\n",
      "[1,    41] loss: 34.46383\n",
      "41\n",
      "[1,    42] loss: 33.65379\n",
      "42\n",
      "[1,    43] loss: 33.89845\n",
      "43\n",
      "[1,    44] loss: 33.17442\n",
      "44\n",
      "[1,    45] loss: 32.75003\n",
      "45\n",
      "[1,    46] loss: 33.04858\n",
      "46\n",
      "[1,    47] loss: 32.73255\n",
      "47\n",
      "[1,    48] loss: 32.42683\n",
      "48\n",
      "[1,    49] loss: 32.13177\n",
      "49\n",
      "[1,    50] loss: 31.80650\n",
      "50\n",
      "[1,    51] loss: 31.60881\n",
      "51\n",
      "[1,    52] loss: 31.58057\n",
      "52\n",
      "[1,    53] loss: 30.91357\n",
      "53\n",
      "[1,    54] loss: 31.53697\n",
      "54\n",
      "[1,    55] loss: 31.22953\n",
      "55\n",
      "[1,    56] loss: 30.70082\n",
      "56\n",
      "[1,    57] loss: 31.12950\n",
      "57\n",
      "[1,    58] loss: 31.26403\n",
      "58\n",
      "[1,    59] loss: 31.07417\n",
      "59\n",
      "[1,    60] loss: 30.61618\n",
      "60\n",
      "[1,    61] loss: 30.61437\n",
      "61\n",
      "[1,    62] loss: 30.19035\n",
      "62\n",
      "[1,    63] loss: 29.47479\n",
      "63\n",
      "[1,    64] loss: 29.27334\n",
      "64\n",
      "[1,    65] loss: 29.23950\n",
      "65\n",
      "[1,    66] loss: 30.10965\n",
      "66\n",
      "[1,    67] loss: 29.87304\n",
      "67\n",
      "[1,    68] loss: 30.98267\n",
      "68\n",
      "[1,    69] loss: 30.50922\n",
      "69\n",
      "[1,    70] loss: 30.97232\n",
      "70\n",
      "[1,    71] loss: 30.87033\n",
      "71\n",
      "[1,    72] loss: 30.96581\n",
      "72\n",
      "[1,    73] loss: 30.32059\n",
      "73\n",
      "[1,    74] loss: 28.88799\n",
      "74\n",
      "[1,    75] loss: 29.05054\n",
      "75\n",
      "[1,    76] loss: 29.19979\n",
      "76\n",
      "[1,    77] loss: 29.49270\n",
      "77\n",
      "[1,    78] loss: 30.18509\n",
      "78\n",
      "[1,    79] loss: 30.32385\n",
      "79\n",
      "[1,    80] loss: 28.91491\n",
      "80\n",
      "[1,    81] loss: 28.23785\n",
      "81\n",
      "[1,    82] loss: 27.81814\n",
      "82\n",
      "[1,    83] loss: 27.98476\n",
      "83\n",
      "[1,    84] loss: 27.59922\n",
      "84\n",
      "[1,    85] loss: 28.16665\n",
      "85\n",
      "[1,    86] loss: 27.66430\n",
      "86\n",
      "[1,    87] loss: 27.55240\n",
      "87\n",
      "[1,    88] loss: 27.47263\n",
      "88\n",
      "[1,    89] loss: 27.53264\n",
      "89\n",
      "[1,    90] loss: 27.72450\n",
      "90\n",
      "[1,    91] loss: 27.52458\n",
      "91\n",
      "[1,    92] loss: 27.50515\n",
      "92\n",
      "[1,    93] loss: 27.47745\n",
      "93\n",
      "[1,    94] loss: 28.25764\n",
      "94\n",
      "[1,    95] loss: 28.31083\n",
      "95\n",
      "[1,    96] loss: 28.70059\n",
      "96\n",
      "[1,    97] loss: 28.54951\n",
      "97\n",
      "[1,    98] loss: 26.98283\n",
      "98\n",
      "[1,    99] loss: 27.11100\n",
      "99\n",
      "[1,   100] loss: 27.56194\n",
      "100\n",
      "[1,   101] loss: 28.03745\n",
      "101\n",
      "[1,   102] loss: 27.29078\n",
      "102\n",
      "[1,   103] loss: 27.15521\n",
      "103\n",
      "[1,   104] loss: 27.10633\n",
      "104\n",
      "[1,   105] loss: 26.84350\n",
      "105\n",
      "[1,   106] loss: 25.75637\n",
      "106\n",
      "[1,   107] loss: 25.83655\n",
      "107\n",
      "[1,   108] loss: 25.79975\n",
      "108\n",
      "[1,   109] loss: 25.36670\n",
      "109\n",
      "[1,   110] loss: 24.92698\n",
      "110\n",
      "[1,   111] loss: 25.30791\n",
      "111\n",
      "[1,   112] loss: 24.96202\n",
      "112\n",
      "[1,   113] loss: 25.08474\n",
      "113\n",
      "[1,   114] loss: 25.93999\n",
      "114\n",
      "[1,   115] loss: 25.76465\n",
      "115\n",
      "[1,   116] loss: 25.78364\n",
      "116\n",
      "[1,   117] loss: 25.81034\n",
      "117\n",
      "[1,   118] loss: 25.61916\n",
      "118\n",
      "[1,   119] loss: 25.51695\n",
      "119\n",
      "[1,   120] loss: 25.95695\n",
      "120\n",
      "[1,   121] loss: 25.85382\n",
      "121\n",
      "[1,   122] loss: 26.57208\n",
      "122\n",
      "[1,   123] loss: 25.86281\n",
      "123\n",
      "[1,   124] loss: 25.94187\n",
      "124\n",
      "[1,   125] loss: 25.29639\n",
      "125\n",
      "[1,   126] loss: 25.25163\n",
      "126\n",
      "[1,   127] loss: 24.37958\n",
      "127\n",
      "[1,   128] loss: 23.88752\n",
      "128\n",
      "[1,   129] loss: 24.00418\n",
      "129\n",
      "[1,   130] loss: 23.83715\n",
      "130\n",
      "[1,   131] loss: 23.71950\n",
      "131\n",
      "[1,   132] loss: 25.26567\n",
      "132\n",
      "[1,   133] loss: 25.00010\n",
      "133\n",
      "[1,   134] loss: 24.97496\n",
      "134\n",
      "[1,   135] loss: 24.98416\n",
      "135\n",
      "[1,   136] loss: 24.42894\n",
      "136\n",
      "[1,   137] loss: 24.07257\n",
      "137\n",
      "[1,   138] loss: 23.63608\n",
      "138\n",
      "[1,   139] loss: 23.72889\n",
      "139\n",
      "[1,   140] loss: 23.22838\n",
      "140\n",
      "[1,   141] loss: 23.21839\n",
      "141\n",
      "[1,   142] loss: 23.38392\n",
      "142\n",
      "[1,   143] loss: 23.55073\n",
      "143\n",
      "[1,   144] loss: 23.58202\n",
      "144\n",
      "[1,   145] loss: 23.46442\n",
      "145\n",
      "[1,   146] loss: 23.90396\n",
      "146\n",
      "[1,   147] loss: 24.47654\n",
      "147\n",
      "[1,   148] loss: 23.82112\n",
      "148\n",
      "[1,   149] loss: 23.72487\n",
      "149\n",
      "[1,   150] loss: 23.14395\n",
      "150\n",
      "[1,   151] loss: 23.45595\n",
      "151\n",
      "[1,   152] loss: 23.61258\n",
      "152\n",
      "[1,   153] loss: 23.39666\n",
      "153\n",
      "[1,   154] loss: 24.20668\n",
      "154\n",
      "[1,   155] loss: 23.78388\n",
      "155\n",
      "[1,   156] loss: 23.31940\n",
      "0\n",
      "[2,     1] loss: 22.48375\n",
      "1\n",
      "[2,     2] loss: 22.03659\n",
      "2\n",
      "[2,     3] loss: 22.23103\n",
      "3\n",
      "[2,     4] loss: 22.59064\n",
      "4\n",
      "[2,     5] loss: 22.38587\n",
      "5\n",
      "[2,     6] loss: 22.30948\n",
      "6\n",
      "[2,     7] loss: 22.10447\n",
      "7\n",
      "[2,     8] loss: 22.58831\n",
      "8\n",
      "[2,     9] loss: 23.22996\n",
      "9\n",
      "[2,    10] loss: 22.95254\n",
      "10\n",
      "[2,    11] loss: 23.01041\n",
      "11\n",
      "[2,    12] loss: 21.51546\n",
      "12\n",
      "[2,    13] loss: 21.77703\n",
      "13\n",
      "[2,    14] loss: 21.05044\n",
      "14\n",
      "[2,    15] loss: 21.53184\n",
      "15\n",
      "[2,    16] loss: 21.63929\n",
      "16\n",
      "[2,    17] loss: 21.45613\n",
      "17\n",
      "[2,    18] loss: 21.92688\n",
      "18\n",
      "[2,    19] loss: 21.89110\n",
      "19\n",
      "[2,    20] loss: 22.54221\n",
      "20\n",
      "[2,    21] loss: 22.03319\n",
      "21\n",
      "[2,    22] loss: 21.59895\n",
      "22\n",
      "[2,    23] loss: 22.11185\n",
      "23\n",
      "[2,    24] loss: 22.53527\n",
      "24\n",
      "[2,    25] loss: 22.15408\n",
      "25\n",
      "[2,    26] loss: 22.74014\n",
      "26\n",
      "[2,    27] loss: 22.07045\n",
      "27\n",
      "[2,    28] loss: 21.21975\n",
      "28\n",
      "[2,    29] loss: 21.57800\n",
      "29\n",
      "[2,    30] loss: 21.19802\n",
      "30\n",
      "[2,    31] loss: 21.29485\n",
      "31\n",
      "[2,    32] loss: 21.70893\n",
      "32\n",
      "[2,    33] loss: 20.61254\n",
      "33\n",
      "[2,    34] loss: 20.79431\n",
      "34\n",
      "[2,    35] loss: 20.25944\n",
      "35\n",
      "[2,    36] loss: 20.45142\n",
      "36\n",
      "[2,    37] loss: 20.36384\n",
      "37\n",
      "[2,    38] loss: 20.88395\n",
      "38\n",
      "[2,    39] loss: 20.47884\n",
      "39\n",
      "[2,    40] loss: 20.27660\n",
      "40\n",
      "[2,    41] loss: 20.14796\n",
      "41\n",
      "[2,    42] loss: 19.76497\n",
      "42\n",
      "[2,    43] loss: 20.07175\n",
      "43\n",
      "[2,    44] loss: 20.13497\n",
      "44\n",
      "[2,    45] loss: 19.25150\n",
      "45\n",
      "[2,    46] loss: 20.40258\n",
      "46\n",
      "[2,    47] loss: 20.04370\n",
      "47\n",
      "[2,    48] loss: 20.17469\n",
      "48\n",
      "[2,    49] loss: 19.91989\n",
      "49\n",
      "[2,    50] loss: 19.48659\n",
      "50\n",
      "[2,    51] loss: 19.30443\n",
      "51\n",
      "[2,    52] loss: 19.72273\n",
      "52\n",
      "[2,    53] loss: 19.26802\n",
      "53\n",
      "[2,    54] loss: 19.72387\n",
      "54\n",
      "[2,    55] loss: 19.60948\n",
      "55\n",
      "[2,    56] loss: 19.20954\n",
      "56\n",
      "[2,    57] loss: 19.57017\n",
      "57\n",
      "[2,    58] loss: 19.52067\n",
      "58\n",
      "[2,    59] loss: 19.71135\n",
      "59\n",
      "[2,    60] loss: 19.27501\n",
      "60\n",
      "[2,    61] loss: 19.48531\n",
      "61\n",
      "[2,    62] loss: 19.20271\n",
      "62\n",
      "[2,    63] loss: 18.70896\n",
      "63\n",
      "[2,    64] loss: 18.52159\n",
      "64\n",
      "[2,    65] loss: 18.64114\n",
      "65\n",
      "[2,    66] loss: 19.27946\n",
      "66\n",
      "[2,    67] loss: 19.23526\n",
      "67\n",
      "[2,    68] loss: 19.88525\n",
      "68\n",
      "[2,    69] loss: 19.48466\n",
      "69\n",
      "[2,    70] loss: 19.96265\n",
      "70\n",
      "[2,    71] loss: 19.95722\n",
      "71\n",
      "[2,    72] loss: 20.01595\n",
      "72\n",
      "[2,    73] loss: 19.60046\n",
      "73\n",
      "[2,    74] loss: 19.00239\n",
      "74\n",
      "[2,    75] loss: 19.18750\n",
      "75\n",
      "[2,    76] loss: 19.45472\n",
      "76\n",
      "[2,    77] loss: 19.56064\n",
      "77\n",
      "[2,    78] loss: 19.93609\n",
      "78\n",
      "[2,    79] loss: 20.00876\n",
      "79\n",
      "[2,    80] loss: 18.88626\n",
      "80\n",
      "[2,    81] loss: 18.32995\n",
      "81\n",
      "[2,    82] loss: 18.24733\n",
      "82\n",
      "[2,    83] loss: 18.51336\n",
      "83\n",
      "[2,    84] loss: 17.81108\n",
      "84\n",
      "[2,    85] loss: 18.37355\n",
      "85\n",
      "[2,    86] loss: 17.91064\n",
      "86\n",
      "[2,    87] loss: 17.78889\n",
      "87\n",
      "[2,    88] loss: 18.00502\n",
      "88\n",
      "[2,    89] loss: 18.29218\n",
      "89\n",
      "[2,    90] loss: 18.42908\n",
      "90\n",
      "[2,    91] loss: 18.12802\n",
      "91\n",
      "[2,    92] loss: 18.33800\n",
      "92\n",
      "[2,    93] loss: 18.22267\n",
      "93\n",
      "[2,    94] loss: 19.17937\n",
      "94\n",
      "[2,    95] loss: 19.27783\n",
      "95\n",
      "[2,    96] loss: 19.74416\n",
      "96\n",
      "[2,    97] loss: 19.72431\n",
      "97\n",
      "[2,    98] loss: 17.71279\n",
      "98\n",
      "[2,    99] loss: 18.02507\n",
      "99\n",
      "[2,   100] loss: 18.16538\n",
      "100\n",
      "[2,   101] loss: 18.60598\n",
      "101\n",
      "[2,   102] loss: 18.39278\n",
      "102\n",
      "[2,   103] loss: 18.13816\n",
      "103\n",
      "[2,   104] loss: 17.93899\n",
      "104\n",
      "[2,   105] loss: 18.01248\n",
      "105\n",
      "[2,   106] loss: 17.41555\n",
      "106\n",
      "[2,   107] loss: 17.44364\n",
      "107\n",
      "[2,   108] loss: 17.43811\n",
      "108\n",
      "[2,   109] loss: 17.46886\n",
      "109\n",
      "[2,   110] loss: 17.03415\n",
      "110\n",
      "[2,   111] loss: 17.26257\n",
      "111\n",
      "[2,   112] loss: 16.91876\n",
      "112\n",
      "[2,   113] loss: 17.21933\n",
      "113\n",
      "[2,   114] loss: 17.83638\n",
      "114\n",
      "[2,   115] loss: 17.58807\n",
      "115\n",
      "[2,   116] loss: 17.36502\n",
      "116\n",
      "[2,   117] loss: 17.42647\n",
      "117\n",
      "[2,   118] loss: 17.56220\n",
      "118\n",
      "[2,   119] loss: 17.30027\n",
      "119\n",
      "[2,   120] loss: 17.62630\n",
      "120\n",
      "[2,   121] loss: 17.44787\n",
      "121\n",
      "[2,   122] loss: 18.45219\n",
      "122\n",
      "[2,   123] loss: 17.72820\n",
      "123\n",
      "[2,   124] loss: 17.89956\n",
      "124\n",
      "[2,   125] loss: 17.74380\n",
      "125\n",
      "[2,   126] loss: 17.73246\n",
      "126\n",
      "[2,   127] loss: 17.20963\n",
      "127\n",
      "[2,   128] loss: 16.69322\n",
      "128\n",
      "[2,   129] loss: 16.67971\n",
      "129\n",
      "[2,   130] loss: 16.40285\n",
      "130\n",
      "[2,   131] loss: 16.46533\n",
      "131\n",
      "[2,   132] loss: 17.61026\n",
      "132\n",
      "[2,   133] loss: 17.32629\n",
      "133\n",
      "[2,   134] loss: 17.15041\n",
      "134\n",
      "[2,   135] loss: 17.35030\n",
      "135\n",
      "[2,   136] loss: 16.92592\n",
      "136\n",
      "[2,   137] loss: 16.85591\n",
      "137\n",
      "[2,   138] loss: 16.33986\n",
      "138\n",
      "[2,   139] loss: 16.55439\n",
      "139\n",
      "[2,   140] loss: 16.10101\n",
      "140\n",
      "[2,   141] loss: 16.13257\n",
      "141\n",
      "[2,   142] loss: 16.37308\n",
      "142\n",
      "[2,   143] loss: 16.50645\n",
      "143\n",
      "[2,   144] loss: 16.56974\n",
      "144\n",
      "[2,   145] loss: 16.45700\n",
      "145\n",
      "[2,   146] loss: 16.76425\n",
      "146\n",
      "[2,   147] loss: 16.92863\n",
      "147\n",
      "[2,   148] loss: 16.52925\n",
      "148\n",
      "[2,   149] loss: 16.53902\n",
      "149\n",
      "[2,   150] loss: 15.80303\n",
      "150\n",
      "[2,   151] loss: 16.68024\n",
      "151\n",
      "[2,   152] loss: 17.25793\n",
      "152\n",
      "[2,   153] loss: 16.97086\n",
      "153\n",
      "[2,   154] loss: 17.52882\n",
      "154\n",
      "[2,   155] loss: 17.26425\n",
      "155\n",
      "[2,   156] loss: 17.37351\n",
      "0\n",
      "[3,     1] loss: 15.99567\n",
      "1\n",
      "[3,     2] loss: 15.61175\n",
      "2\n",
      "[3,     3] loss: 15.70813\n",
      "3\n",
      "[3,     4] loss: 15.77126\n",
      "4\n",
      "[3,     5] loss: 15.54676\n",
      "5\n",
      "[3,     6] loss: 15.49404\n",
      "6\n",
      "[3,     7] loss: 15.39806\n",
      "7\n",
      "[3,     8] loss: 16.22513\n",
      "8\n",
      "[3,     9] loss: 16.63024\n",
      "9\n",
      "[3,    10] loss: 16.47585\n",
      "10\n",
      "[3,    11] loss: 16.54813\n",
      "11\n",
      "[3,    12] loss: 15.30507\n",
      "12\n",
      "[3,    13] loss: 15.68546\n",
      "13\n",
      "[3,    14] loss: 14.82854\n",
      "14\n",
      "[3,    15] loss: 15.30283\n",
      "15\n",
      "[3,    16] loss: 15.35578\n",
      "16\n",
      "[3,    17] loss: 15.17709\n",
      "17\n",
      "[3,    18] loss: 15.65625\n",
      "18\n",
      "[3,    19] loss: 15.76643\n",
      "19\n",
      "[3,    20] loss: 16.29210\n",
      "20\n",
      "[3,    21] loss: 15.87208\n",
      "21\n",
      "[3,    22] loss: 15.49369\n",
      "22\n",
      "[3,    23] loss: 15.87007\n",
      "23\n",
      "[3,    24] loss: 16.18871\n",
      "24\n",
      "[3,    25] loss: 15.99238\n",
      "25\n",
      "[3,    26] loss: 16.54489\n",
      "26\n",
      "[3,    27] loss: 15.87866\n",
      "27\n",
      "[3,    28] loss: 15.17833\n",
      "28\n",
      "[3,    29] loss: 15.54728\n",
      "29\n",
      "[3,    30] loss: 15.32247\n",
      "30\n",
      "[3,    31] loss: 15.34137\n",
      "31\n",
      "[3,    32] loss: 15.72202\n",
      "32\n",
      "[3,    33] loss: 14.96111\n",
      "33\n",
      "[3,    34] loss: 15.02267\n",
      "34\n",
      "[3,    35] loss: 14.67857\n",
      "35\n",
      "[3,    36] loss: 14.91546\n",
      "36\n",
      "[3,    37] loss: 14.95535\n",
      "37\n",
      "[3,    38] loss: 15.21634\n",
      "38\n",
      "[3,    39] loss: 14.97745\n",
      "39\n",
      "[3,    40] loss: 14.78381\n",
      "40\n",
      "[3,    41] loss: 14.79967\n",
      "41\n",
      "[3,    42] loss: 14.52001\n",
      "42\n",
      "[3,    43] loss: 14.80840\n",
      "43\n",
      "[3,    44] loss: 14.98505\n",
      "44\n",
      "[3,    45] loss: 14.05971\n",
      "45\n",
      "[3,    46] loss: 15.22524\n",
      "46\n",
      "[3,    47] loss: 14.90689\n",
      "47\n",
      "[3,    48] loss: 15.11372\n",
      "48\n",
      "[3,    49] loss: 14.91885\n",
      "49\n",
      "[3,    50] loss: 14.45573\n",
      "50\n",
      "[3,    51] loss: 14.21608\n",
      "51\n",
      "[3,    52] loss: 14.60935\n",
      "52\n",
      "[3,    53] loss: 14.34045\n",
      "53\n",
      "[3,    54] loss: 14.68724\n",
      "54\n",
      "[3,    55] loss: 14.64996\n",
      "55\n",
      "[3,    56] loss: 14.34222\n",
      "56\n",
      "[3,    57] loss: 14.70955\n",
      "57\n",
      "[3,    58] loss: 14.68201\n",
      "58\n",
      "[3,    59] loss: 14.89666\n",
      "59\n",
      "[3,    60] loss: 14.51682\n",
      "60\n",
      "[3,    61] loss: 14.67290\n",
      "61\n",
      "[3,    62] loss: 14.47854\n",
      "62\n",
      "[3,    63] loss: 14.07810\n",
      "63\n",
      "[3,    64] loss: 13.92336\n",
      "64\n",
      "[3,    65] loss: 14.10729\n",
      "65\n",
      "[3,    66] loss: 14.39157\n",
      "66\n",
      "[3,    67] loss: 14.34213\n",
      "67\n",
      "[3,    68] loss: 14.87436\n",
      "68\n",
      "[3,    69] loss: 14.59146\n",
      "69\n",
      "[3,    70] loss: 14.98458\n",
      "70\n",
      "[3,    71] loss: 15.03423\n",
      "71\n",
      "[3,    72] loss: 15.08271\n",
      "72\n",
      "[3,    73] loss: 14.77140\n",
      "73\n",
      "[3,    74] loss: 14.68057\n",
      "74\n",
      "[3,    75] loss: 14.74255\n",
      "75\n",
      "[3,    76] loss: 15.03647\n",
      "76\n",
      "[3,    77] loss: 15.11744\n",
      "77\n",
      "[3,    78] loss: 15.27969\n",
      "78\n",
      "[3,    79] loss: 15.39733\n",
      "79\n",
      "[3,    80] loss: 14.45332\n",
      "80\n",
      "[3,    81] loss: 13.85799\n",
      "81\n",
      "[3,    82] loss: 13.93936\n",
      "82\n",
      "[3,    83] loss: 14.20870\n",
      "83\n",
      "[3,    84] loss: 13.57621\n",
      "84\n",
      "[3,    85] loss: 14.08934\n",
      "85\n",
      "[3,    86] loss: 13.62221\n",
      "86\n",
      "[3,    87] loss: 13.44851\n",
      "87\n",
      "[3,    88] loss: 13.82204\n",
      "88\n",
      "[3,    89] loss: 14.14607\n",
      "89\n",
      "[3,    90] loss: 14.16173\n",
      "90\n",
      "[3,    91] loss: 14.01066\n",
      "91\n",
      "[3,    92] loss: 14.24116\n",
      "92\n",
      "[3,    93] loss: 14.10063\n",
      "93\n",
      "[3,    94] loss: 14.98977\n",
      "94\n",
      "[3,    95] loss: 15.13591\n",
      "95\n",
      "[3,    96] loss: 15.56023\n",
      "96\n",
      "[3,    97] loss: 15.55601\n",
      "97\n",
      "[3,    98] loss: 13.71046\n",
      "98\n",
      "[3,    99] loss: 14.03086\n",
      "99\n",
      "[3,   100] loss: 13.91873\n",
      "100\n",
      "[3,   101] loss: 14.50103\n",
      "101\n",
      "[3,   102] loss: 14.30017\n",
      "102\n",
      "[3,   103] loss: 14.06449\n",
      "103\n",
      "[3,   104] loss: 13.93601\n",
      "104\n",
      "[3,   105] loss: 13.97931\n",
      "105\n",
      "[3,   106] loss: 13.62502\n",
      "106\n",
      "[3,   107] loss: 13.69467\n",
      "107\n",
      "[3,   108] loss: 13.69861\n",
      "108\n",
      "[3,   109] loss: 13.68254\n",
      "109\n",
      "[3,   110] loss: 13.23292\n",
      "110\n",
      "[3,   111] loss: 13.45728\n",
      "111\n",
      "[3,   112] loss: 13.17474\n",
      "112\n",
      "[3,   113] loss: 13.45569\n",
      "113\n",
      "[3,   114] loss: 14.09402\n",
      "114\n",
      "[3,   115] loss: 13.75672\n",
      "115\n",
      "[3,   116] loss: 13.49591\n",
      "116\n",
      "[3,   117] loss: 13.53201\n",
      "117\n",
      "[3,   118] loss: 13.79496\n",
      "118\n",
      "[3,   119] loss: 13.49860\n",
      "119\n",
      "[3,   120] loss: 13.80605\n",
      "120\n",
      "[3,   121] loss: 13.62380\n",
      "121\n",
      "[3,   122] loss: 14.51790\n",
      "122\n",
      "[3,   123] loss: 13.93055\n",
      "123\n",
      "[3,   124] loss: 14.13611\n",
      "124\n",
      "[3,   125] loss: 14.08038\n",
      "125\n",
      "[3,   126] loss: 14.14875\n",
      "126\n",
      "[3,   127] loss: 13.73022\n",
      "127\n",
      "[3,   128] loss: 13.24765\n",
      "128\n",
      "[3,   129] loss: 13.22954\n",
      "129\n",
      "[3,   130] loss: 12.96232\n",
      "130\n",
      "[3,   131] loss: 13.08371\n",
      "131\n",
      "[3,   132] loss: 14.04572\n",
      "132\n",
      "[3,   133] loss: 13.78912\n",
      "133\n",
      "[3,   134] loss: 13.55361\n",
      "134\n",
      "[3,   135] loss: 13.75035\n",
      "135\n",
      "[3,   136] loss: 13.37847\n",
      "136\n",
      "[3,   137] loss: 13.50129\n",
      "137\n",
      "[3,   138] loss: 12.95295\n",
      "138\n",
      "[3,   139] loss: 13.21940\n",
      "139\n",
      "[3,   140] loss: 12.81143\n",
      "140\n",
      "[3,   141] loss: 12.84421\n",
      "141\n",
      "[3,   142] loss: 13.11094\n",
      "142\n",
      "[3,   143] loss: 13.35113\n",
      "143\n",
      "[3,   144] loss: 13.44924\n",
      "144\n",
      "[3,   145] loss: 13.32069\n",
      "145\n",
      "[3,   146] loss: 13.52228\n",
      "146\n",
      "[3,   147] loss: 13.32479\n",
      "147\n",
      "[3,   148] loss: 13.10169\n",
      "148\n",
      "[3,   149] loss: 13.17415\n",
      "149\n",
      "[3,   150] loss: 12.44242\n",
      "150\n",
      "[3,   151] loss: 13.47214\n",
      "151\n",
      "[3,   152] loss: 14.18217\n",
      "152\n",
      "[3,   153] loss: 13.77138\n",
      "153\n",
      "[3,   154] loss: 14.23373\n",
      "154\n",
      "[3,   155] loss: 14.07371\n",
      "155\n",
      "[3,   156] loss: 14.80507\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "shakespeare_dataloader = DataLoader(shakespeare_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "config = TransformerConfig(\n",
    "    vocab_size=shakespeare_dataset.vocab_size,\n",
    "    hidden_size=256,\n",
    "    num_heads=4,\n",
    "    num_layers=2\n",
    ")\n",
    "\n",
    "model = DecoderTransformer(config)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "accuracy_list = train(config, model, shakespeare_dataloader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderTransformer(\n",
       "  (decoderlayer): Sequential(\n",
       "    (decoder0): DecoderBlock(\n",
       "      (attention): MultiheadMaskedAttention(\n",
       "        (W_QKV): Linear(in_features=256, out_features=768, bias=True)\n",
       "        (W_O): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (layernorm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (layernorm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MultiLayerPerceptron(\n",
       "        (model): Sequential(\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (GELU): GELU(approximate=none)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder1): DecoderBlock(\n",
       "      (attention): MultiheadMaskedAttention(\n",
       "        (W_QKV): Linear(in_features=256, out_features=768, bias=True)\n",
       "        (W_O): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (layernorm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (layernorm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MultiLayerPerceptron(\n",
       "        (model): Sequential(\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (GELU): GELU(approximate=none)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (embed): Embedding(34543, 256)\n",
       "  (positional_embedding): PositionalEncoding()\n",
       "  (last_linear): Linear(in_features=256, out_features=34543, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a41fdc720b403cff5d22ec3440153970555b5fcc336583b0458a17a41b31d53f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
