{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from fancy_einsum import einsum\n",
    "from einops import rearrange, repeat\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "def singlehead_attention(Q: t.Tensor, K: t.Tensor, V: t.Tensor):\n",
    "    '''\n",
    "    Should return the results of self-attention (see the \"Self-Attention in Detail\" section of the Illustrated Transformer).\n",
    "\n",
    "    With this function, you can ignore masking.\n",
    "\n",
    "    Q: shape (b, s, c)\n",
    "    K: shape (b, s, c)\n",
    "    V: shape (b, s, c)\n",
    "    b = batch\n",
    "    s = seq_len\n",
    "    c = dims\n",
    "\n",
    "    Return: shape (b s s)\n",
    "    '''\n",
    "    d_k = math.sqrt(Q.shape[-1])\n",
    "    scaled_dot_prod: Tensor = einsum('b s1 c, b s2 c -> b s1 s2', Q, K) / d_k\n",
    "    return scaled_dot_prod.softmax(dim=-1) @ V\n",
    "\n",
    "def masked_attention(Q: t.Tensor, K: t.Tensor, V: t.Tensor, mask: t.Tensor):\n",
    "    '''\n",
    "    Q: shape (b, s, c)\n",
    "    K: shape (b, s, c)\n",
    "    V: shape (b, s, c)\n",
    "    mask: shape (b, s, s)\n",
    "    b = batch\n",
    "    s = seq_len\n",
    "    c = dims\n",
    "\n",
    "    Return: shape (b s s)\n",
    "    '''\n",
    "    d_k = math.sqrt(Q.shape[-1])\n",
    "    scaled_dot_prod: Tensor = einsum('b s1 c, b s2 c -> b s1 s2', Q, K) / d_k\n",
    "    if mask is not None:\n",
    "        scaled_dot_prod = scaled_dot_prod.masked_fill(mask == 0, -1e9)\n",
    "    return scaled_dot_prod.softmax(dim=-1) @ V\n",
    "\n",
    "def multihead_attention(Q: t.Tensor, K: t.Tensor, V: t.Tensor, n_heads: int):\n",
    "    '''\n",
    "    Q: shape (b, s1, e)\n",
    "    K: shape (b, s2, e)\n",
    "    V: shape (b, s2, e)\n",
    "\n",
    "    e = nheads * h\n",
    "    b = batch\n",
    "    s = seq_len\n",
    "    h = hidden\n",
    "\n",
    "    Return: shape (b s e)\n",
    "    '''\n",
    "\n",
    "    assert Q.shape[-1] % n_heads == 0\n",
    "    assert K.shape[-1] % n_heads == 0\n",
    "    assert V.shape[-1] % n_heads == 0\n",
    "    assert K.shape[-1] == V.shape[-1]\n",
    "\n",
    "    # mask for autoencoder\n",
    "    mask = t.triu(t.ones(Q.shape[1], K.shape[1]), diagonal=1).bool()\n",
    "\n",
    "    Q = rearrange(Q, 'b s (nheads h) -> b nheads s h', nheads=n_heads)\n",
    "    K = rearrange(K, 'b s (nheads h) -> b nheads s h', nheads=n_heads)\n",
    "    V = rearrange(V, 'b s (nheads h) -> b nheads s h', nheads=n_heads)\n",
    "\n",
    "    scaled_dot_prod = einsum('b nheads s1 h, b nheads s2 h -> b nheads s2 s1', K, Q) / math.sqrt(Q.shape[-1])\n",
    "    # if mask is not None:\n",
    "    #     if mask.dim() == 2:\n",
    "    #         mask = repeat(mask, 's1 s2 -> b s1 s2', b=Q.shape[0])\n",
    "    #     else:\n",
    "    #         mask = mask.unsqueeze(1)\n",
    "    #     # print(mask.shape, scaled_dot_prod.shape)\n",
    "    #     scaled_dot_prod = scaled_dot_prod.masked_fill(mask == 1, -1e9)\n",
    "    mask_filter = t.triu(t.full_like(scaled_dot_prod, -t.inf), 1)\n",
    "    scaled_dot_prod += mask_filter\n",
    "    attention_probs = scaled_dot_prod.softmax(dim=-1)\n",
    "    attention_vals = einsum('b nheads s1 s2, b nheads s2 c -> b nheads s1 c', attention_probs, V)\n",
    "    attention = rearrange(attention_vals, 'b nheads s c -> b s (nheads c)')\n",
    "    return attention\n",
    "\n",
    "class MultiheadMaskedAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.W_QKV = nn.Linear(hidden_size, hidden_size * 3)\n",
    "        self.W_O = nn.Linear(hidden_size, hidden_size)\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def forward(self, x: t.Tensor, mask=None) -> t.Tensor:\n",
    "        '''\n",
    "        x: shape (batch, seq, hidden_size)\n",
    "        Return: shape (batch, seq, hidden_size)\n",
    "        '''\n",
    "        Q, K, V = self.W_QKV(x).chunk(3, dim=-1)\n",
    "        att = multihead_attention(Q, K, V, self.num_heads)\n",
    "        return self.W_O(att)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[14.2070, 15.0070, 15.8070],\n",
      "         [14.3999, 15.1999, 15.9999],\n",
      "         [14.4000, 15.2000, 16.0000],\n",
      "         [14.4000, 15.2000, 16.0000],\n",
      "         [14.4000, 15.2000, 16.0000],\n",
      "         [14.4000, 15.2000, 16.0000],\n",
      "         [14.4000, 15.2000, 16.0000]],\n",
      "\n",
      "        [[31.2000, 32.0000, 32.8000],\n",
      "         [31.2000, 32.0000, 32.8000],\n",
      "         [31.2000, 32.0000, 32.8000],\n",
      "         [31.2000, 32.0000, 32.8000],\n",
      "         [31.2000, 32.0000, 32.8000],\n",
      "         [31.2000, 32.0000, 32.8000],\n",
      "         [31.2000, 32.0000, 32.8000]]])\n"
     ]
    }
   ],
   "source": [
    "# test single_head_attention\n",
    "Q = t.arange(2 * 7 * 3).reshape(2, 7, 3).type(t.float32)\n",
    "K = Q * 0.5\n",
    "V = Q * 0.8\n",
    "print(singlehead_attention(Q,K,V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[15.0000],\n",
       "         [14.5878],\n",
       "         [13.9747],\n",
       "         [13.2046],\n",
       "         [12.4225],\n",
       "         [11.6769],\n",
       "         [10.9564],\n",
       "         [10.2500],\n",
       "         [ 9.5519],\n",
       "         [ 8.8588]],\n",
       "\n",
       "        [[ 8.1579],\n",
       "         [ 7.4807],\n",
       "         [ 6.7942],\n",
       "         [ 6.1084],\n",
       "         [ 5.4231],\n",
       "         [ 4.7382],\n",
       "         [ 4.0535],\n",
       "         [ 3.3690],\n",
       "         [ 2.6846],\n",
       "         [ 2.0003]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = t.linspace(0, 10, 2 * 10 * 1).reshape(2, 10, 1)\n",
    "K = t.linspace(5, 20, 2 * 10 * 1).reshape(2, 10, 1)\n",
    "V = t.linspace(15, 2, 2 * 10 * 1).reshape(2, 10, 1)\n",
    "# b = 2, s = 5, c = 4\n",
    "multihead_attention(Q, K, V, n_heads=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -0.7193,   0.4614,   0.4117,  -0.5813,   0.2754,  -0.5745],\n",
       "         [ -0.7746,   0.6206,   0.5520,  -0.7370,   0.1787,  -0.7289],\n",
       "         [ -1.1632,   1.7392,   1.5775,  -1.7907,  -0.5079,  -1.8103]],\n",
       "\n",
       "        [[  0.0549,  -1.9665, -10.8756,  -7.1792,   3.4559,   0.9521],\n",
       "         [ -0.3971,  -0.6652,  -9.6883,  -8.4108,   2.6582,  -0.3063],\n",
       "         [ -0.8686,   0.6920,  -8.4500,  -9.6953,   1.8262,  -1.6189]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.manual_seed(420)\n",
    "m = MultiheadMaskedAttention(6, 2)\n",
    "x = t.linspace(0, 42, 2 * 3 * 6).reshape(2, 3, 6)\n",
    "m(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class TransformerConfig:\n",
    "    '''Constants used throughout your decoder-only transformer model.'''\n",
    "\n",
    "    num_layers: int\n",
    "    num_heads: int\n",
    "    vocab_size: int\n",
    "    hidden_size: int # also embedding dim or d_model\n",
    "    max_seq_len: int = 5000 \n",
    "    dropout: float = 0.1\n",
    "    layer_norm_epsilon: float = 1e-05\n",
    "    device = 'cpu'\n",
    "\n",
    "config = TransformerConfig(\n",
    "    num_layers = 6,\n",
    "    num_heads = 4,\n",
    "    vocab_size = 10,\n",
    "    hidden_size = 96\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "\n",
    "        d = d_model\n",
    "        L = max_len\n",
    "        D = d / 2\n",
    "\n",
    "        angles = t.outer(t.arange(L), 1 / 10000 ** (2 * t.arange(D) / D))\n",
    "\n",
    "        array_2d = t.zeros((L, d))\n",
    "        array_2d[:, ::2] = t.sin(angles)\n",
    "        array_2d[:, 1::2] = t.cos(angles)\n",
    "        self.encoding = array_2d\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        '''\n",
    "        x: Tensor, shape [batch, seq_len, embedding_dim]\n",
    "        ''' \n",
    "        batch_size, seq_len, embedding_dim = x.size()\n",
    "        return self.encoding[:seq_len, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import OrderedDict\n",
    "\n",
    "# class MultiLayerPerceptron(nn.Module):  \n",
    "\n",
    "#     def __init__(self, d_in: int, d_out: int):\n",
    "#         super().__init__()\n",
    "#         d_h = d_in * 4\n",
    "#         self.model = nn.Sequential(OrderedDict([\n",
    "#             ('linear1', nn.Linear(d_in, d_h)),\n",
    "#             ('GELU', nn.GELU()),\n",
    "#             ('linear2', nn.Linear(d_h, d_in)),   \n",
    "#             ('dropout', nn.Dropout(p=0.1))\n",
    "#         ]))\n",
    "\n",
    "#     def forward(self, x: t.Tensor):\n",
    "#         return self.model(x)\n",
    "        \n",
    "# class DecoderBlock(nn.Module):\n",
    "\n",
    "#     def __init__(self, config: TransformerConfig):\n",
    "#         super().__init__()\n",
    "#         self.attention = MultiheadMaskedAttention(\n",
    "#             hidden_size=config.hidden_size,\n",
    "#             num_heads=config.num_heads\n",
    "#         )\n",
    "#         self.layernorm1 = nn.LayerNorm(config.hidden_size)\n",
    "#         self.layernorm2 = nn.LayerNorm(config.hidden_size)\n",
    "#         self.mlp = MultiLayerPerceptron(config.hidden_size, config.hidden_size)\n",
    "    \n",
    "#     def forward(self, x: t.Tensor):\n",
    "#         att = self.attention(x) + x\n",
    "#         h1 = self.layernorm1(att)\n",
    "#         h2 = self.layernorm2(self.mlp(h1) + h1)\n",
    "#         return h2\n",
    "\n",
    "# class DecoderTransformer(nn.Module):\n",
    "\n",
    "#     def __init__(self, config: TransformerConfig):\n",
    "#         super().__init__()\n",
    "#         decoders = [DecoderBlock(config) for i in range(config.num_layers)]\n",
    "#         names = ['decoder' + str(i) for i in range(config.num_layers)]\n",
    "#         self.decoderlayer = nn.Sequential(OrderedDict(zip(names, decoders)))\n",
    "#         self.dropout = nn.Dropout(p=config.dropout)\n",
    "#         self.layernorm = nn.LayerNorm(config.hidden_size) # why? come back to this later\n",
    "#         self.embed = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "#         self.positional_embedding = PositionalEncoding(config.hidden_size)\n",
    "#         self.last_linear = nn.Linear(config.hidden_size, config.vocab_size)\n",
    "\n",
    "#     def forward(self, tokens):\n",
    "#         embedding = self.embed(tokens) # (seq_len) -> (seq_len, embedding)\n",
    "#         pos_embedding = self.positional_embedding(embedding)\n",
    "#         final_embedding = embedding + pos_embedding\n",
    "#         a = self.dropout(final_embedding)\n",
    "#         b = self.decoderlayer(a)\n",
    "#         c = self.layernorm(b) @ self.embed.weight.T\n",
    "#         return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TestDataSet(Dataset):\n",
    "    \"\"\"A toy dataset to train a model to predict\n",
    "     a random sequence of tokens.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.seq_len = 25\n",
    "        self.total_size = 1000\n",
    "        self.text = t.randint(0,config.vocab_size, (self.total_size, self.seq_len)).to(config.device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.total_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.text[idx,1:]\n",
    "        text = self.text[idx,:-1]\n",
    "        return (text, label)\n",
    "\n",
    "class ReversedNumbers(Dataset):\n",
    "    def __init__(self, vocab_size: int, seq_len: int, datasize: int):\n",
    "        self.seqs = t.randint(0, vocab_size, (datasize, seq_len))\n",
    "\n",
    "    def __len__(self):\n",
    "            return len(self.seqs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "            input = self.seqs[idx]\n",
    "            target = t.flip(input, dims=(0,))\n",
    "            return (input, target)\n",
    "\n",
    "# class ShakespeareDataset(Dataset):\n",
    "#     def __init__(self, config):\n",
    "#         self.data = open('shakespeare.txt', 'r').read()\n",
    "#         self.config = config\n",
    "#         chars = sorted(set(self.data))\n",
    "#         self.vocab_size = len(chars)\n",
    "#         self.char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "#         self.idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "#         print('data has %d characters, %d unique.' % (len(self.data), self.vocab_size))\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         x = self.char_to_idx[self.data[index]]\n",
    "#         x = t.tensor([x])\n",
    "#         x = F.one_hot(x, num_classes=self.vocab_size)\n",
    "#         x = x.type(t.FloatTensor)\n",
    "#         t = self.char_to_idx[self.data[index + (index < (self.__len__() - 1))]]\n",
    "#         t = t.tensor([t])\n",
    "#         return (x.to(self.config.device), t.to(self.config.device))\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def params(self):\n",
    "#         return self.vocab_size, self.char_to_idx, self.idx_to_char\n",
    "\n",
    "# torch.utils.data.random_split(dataset, lengths, generator=<torch._C.Generator object>)\n",
    "# dummy_ds = TestDataSet(config)\n",
    "# dummy_dl = DataLoader(dummy_ds, batch_size=64, shuffle=True)\n",
    "nums_ds = ReversedNumbers(vocab_size=10, seq_len=6, datasize=10000)\n",
    "train_ds, val_ds = random_split(nums_ds, [8000, 2000])\n",
    "nums_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "nums_tl = DataLoader(val_ds, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    20] loss: 0.32108\n",
      "[1,    40] loss: 0.07130\n",
      "[1,    60] loss: 0.03608\n",
      "[1,    80] loss: 0.03188\n",
      "[1,   100] loss: 0.03027\n",
      "[1,   120] loss: 0.02934\n",
      "accuracy: 0.095\n",
      "[2,    20] loss: 0.02830\n",
      "[2,    40] loss: 0.02744\n",
      "[2,    60] loss: 0.02707\n",
      "[2,    80] loss: 0.02652\n",
      "[2,   100] loss: 0.02611\n",
      "[2,   120] loss: 0.02578\n",
      "accuracy: 0.103\n",
      "[3,    20] loss: 0.02553\n",
      "[3,    40] loss: 0.02516\n",
      "[3,    60] loss: 0.02489\n",
      "[3,    80] loss: 0.02465\n",
      "[3,   100] loss: 0.02447\n",
      "[3,   120] loss: 0.02427\n",
      "accuracy: 0.109\n",
      "[4,    20] loss: 0.02411\n",
      "[4,    40] loss: 0.02398\n",
      "[4,    60] loss: 0.02383\n",
      "[4,    80] loss: 0.02383\n",
      "[4,   100] loss: 0.02349\n",
      "[4,   120] loss: 0.02355\n",
      "accuracy: 0.133\n",
      "[5,    20] loss: 0.02350\n",
      "[5,    40] loss: 0.02340\n",
      "[5,    60] loss: 0.02331\n",
      "[5,    80] loss: 0.02328\n",
      "[5,   100] loss: 0.02333\n",
      "[5,   120] loss: 0.02309\n",
      "accuracy: 0.145\n",
      "[6,    20] loss: 0.02306\n",
      "[6,    40] loss: 0.02298\n",
      "[6,    60] loss: 0.02289\n",
      "[6,    80] loss: 0.02281\n",
      "[6,   100] loss: 0.02279\n",
      "[6,   120] loss: 0.02266\n",
      "accuracy: 0.178\n",
      "[7,    20] loss: 0.02258\n",
      "[7,    40] loss: 0.02253\n",
      "[7,    60] loss: 0.02254\n",
      "[7,    80] loss: 0.02252\n",
      "[7,   100] loss: 0.02251\n",
      "[7,   120] loss: 0.02236\n",
      "accuracy: 0.217\n",
      "[8,    20] loss: 0.02233\n",
      "[8,    40] loss: 0.02224\n",
      "[8,    60] loss: 0.02220\n",
      "[8,    80] loss: 0.02208\n",
      "[8,   100] loss: 0.02195\n",
      "[8,   120] loss: 0.02200\n",
      "accuracy: 0.232\n",
      "[9,    20] loss: 0.02196\n",
      "[9,    40] loss: 0.02187\n",
      "[9,    60] loss: 0.02201\n",
      "[9,    80] loss: 0.02175\n",
      "[9,   100] loss: 0.02180\n",
      "[9,   120] loss: 0.02174\n",
      "accuracy: 0.234\n",
      "[10,    20] loss: 0.02161\n",
      "[10,    40] loss: 0.02156\n",
      "[10,    60] loss: 0.02146\n",
      "[10,    80] loss: 0.02152\n",
      "[10,   100] loss: 0.02139\n",
      "[10,   120] loss: 0.02137\n",
      "accuracy: 0.235\n",
      "[11,    20] loss: 0.02131\n",
      "[11,    40] loss: 0.02120\n",
      "[11,    60] loss: 0.02120\n",
      "[11,    80] loss: 0.02121\n",
      "[11,   100] loss: 0.02109\n",
      "[11,   120] loss: 0.02106\n",
      "accuracy: 0.231\n",
      "[12,    20] loss: 0.02087\n",
      "[12,    40] loss: 0.02091\n",
      "[12,    60] loss: 0.02086\n",
      "[12,    80] loss: 0.02055\n",
      "[12,   100] loss: 0.02076\n",
      "[12,   120] loss: 0.02069\n",
      "accuracy: 0.237\n",
      "[13,    20] loss: 0.02061\n",
      "[13,    40] loss: 0.02058\n",
      "[13,    60] loss: 0.02044\n",
      "[13,    80] loss: 0.02044\n",
      "[13,   100] loss: 0.02038\n",
      "[13,   120] loss: 0.02022\n",
      "accuracy: 0.237\n",
      "[14,    20] loss: 0.02021\n",
      "[14,    40] loss: 0.02030\n",
      "[14,    60] loss: 0.02017\n",
      "[14,    80] loss: 0.02020\n",
      "[14,   100] loss: 0.02018\n",
      "[14,   120] loss: 0.02007\n",
      "accuracy: 0.241\n",
      "[15,    20] loss: 0.01999\n",
      "[15,    40] loss: 0.01995\n",
      "[15,    60] loss: 0.01985\n",
      "[15,    80] loss: 0.01985\n",
      "[15,   100] loss: 0.01979\n",
      "[15,   120] loss: 0.01974\n",
      "accuracy: 0.241\n",
      "[16,    20] loss: 0.01960\n",
      "[16,    40] loss: 0.01972\n",
      "[16,    60] loss: 0.01948\n",
      "[16,    80] loss: 0.01940\n",
      "[16,   100] loss: 0.01933\n",
      "[16,   120] loss: 0.01946\n",
      "accuracy: 0.242\n",
      "[17,    20] loss: 0.01923\n",
      "[17,    40] loss: 0.01919\n",
      "[17,    60] loss: 0.01918\n",
      "[17,    80] loss: 0.01907\n",
      "[17,   100] loss: 0.01894\n",
      "[17,   120] loss: 0.01888\n",
      "accuracy: 0.244\n",
      "[18,    20] loss: 0.01880\n",
      "[18,    40] loss: 0.01870\n",
      "[18,    60] loss: 0.01869\n",
      "[18,    80] loss: 0.01867\n",
      "[18,   100] loss: 0.01867\n",
      "[18,   120] loss: 0.01864\n",
      "accuracy: 0.251\n",
      "[19,    20] loss: 0.01849\n",
      "[19,    40] loss: 0.01836\n",
      "[19,    60] loss: 0.01840\n",
      "[19,    80] loss: 0.01827\n",
      "[19,   100] loss: 0.01841\n",
      "[19,   120] loss: 0.01825\n",
      "accuracy: 0.265\n",
      "[20,    20] loss: 0.01824\n",
      "[20,    40] loss: 0.01810\n",
      "[20,    60] loss: 0.01808\n",
      "[20,    80] loss: 0.01812\n",
      "[20,   100] loss: 0.01796\n",
      "[20,   120] loss: 0.01792\n",
      "accuracy: 0.278\n",
      "[21,    20] loss: 0.01784\n",
      "[21,    40] loss: 0.01780\n",
      "[21,    60] loss: 0.01789\n",
      "[21,    80] loss: 0.01784\n",
      "[21,   100] loss: 0.01769\n",
      "[21,   120] loss: 0.01764\n",
      "accuracy: 0.288\n",
      "[22,    20] loss: 0.01756\n",
      "[22,    40] loss: 0.01748\n",
      "[22,    60] loss: 0.01750\n",
      "[22,    80] loss: 0.01744\n",
      "[22,   100] loss: 0.01749\n",
      "[22,   120] loss: 0.01729\n",
      "accuracy: 0.304\n",
      "[23,    20] loss: 0.01721\n",
      "[23,    40] loss: 0.01722\n",
      "[23,    60] loss: 0.01719\n",
      "[23,    80] loss: 0.01701\n",
      "[23,   100] loss: 0.01700\n",
      "[23,   120] loss: 0.01704\n",
      "accuracy: 0.328\n",
      "[24,    20] loss: 0.01688\n",
      "[24,    40] loss: 0.01679\n",
      "[24,    60] loss: 0.01681\n",
      "[24,    80] loss: 0.01662\n",
      "[24,   100] loss: 0.01667\n",
      "[24,   120] loss: 0.01664\n",
      "accuracy: 0.351\n",
      "[25,    20] loss: 0.01655\n",
      "[25,    40] loss: 0.01644\n",
      "[25,    60] loss: 0.01643\n",
      "[25,    80] loss: 0.01640\n",
      "[25,   100] loss: 0.01637\n",
      "[25,   120] loss: 0.01625\n",
      "accuracy: 0.366\n",
      "[26,    20] loss: 0.01619\n",
      "[26,    40] loss: 0.01613\n",
      "[26,    60] loss: 0.01606\n",
      "[26,    80] loss: 0.01598\n",
      "[26,   100] loss: 0.01594\n",
      "[26,   120] loss: 0.01596\n",
      "accuracy: 0.387\n",
      "[27,    20] loss: 0.01565\n",
      "[27,    40] loss: 0.01579\n",
      "[27,    60] loss: 0.01556\n",
      "[27,    80] loss: 0.01572\n",
      "[27,   100] loss: 0.01578\n",
      "[27,   120] loss: 0.01569\n",
      "accuracy: 0.399\n",
      "[28,    20] loss: 0.01556\n",
      "[28,    40] loss: 0.01561\n",
      "[28,    60] loss: 0.01545\n",
      "[28,    80] loss: 0.01531\n",
      "[28,   100] loss: 0.01538\n",
      "[28,   120] loss: 0.01527\n",
      "accuracy: 0.419\n",
      "[29,    20] loss: 0.01516\n",
      "[29,    40] loss: 0.01508\n",
      "[29,    60] loss: 0.01507\n",
      "[29,    80] loss: 0.01510\n",
      "[29,   100] loss: 0.01501\n",
      "[29,   120] loss: 0.01491\n",
      "accuracy: 0.430\n",
      "[30,    20] loss: 0.01492\n",
      "[30,    40] loss: 0.01492\n",
      "[30,    60] loss: 0.01494\n",
      "[30,    80] loss: 0.01484\n",
      "[30,   100] loss: 0.01503\n",
      "[30,   120] loss: 0.01486\n",
      "accuracy: 0.434\n",
      "[31,    20] loss: 0.01471\n",
      "[31,    40] loss: 0.01475\n",
      "[31,    60] loss: 0.01480\n",
      "[31,    80] loss: 0.01463\n",
      "[31,   100] loss: 0.01460\n",
      "[31,   120] loss: 0.01443\n",
      "accuracy: 0.441\n",
      "[32,    20] loss: 0.01452\n",
      "[32,    40] loss: 0.01445\n",
      "[32,    60] loss: 0.01451\n",
      "[32,    80] loss: 0.01456\n",
      "[32,   100] loss: 0.01451\n",
      "[32,   120] loss: 0.01441\n",
      "accuracy: 0.458\n",
      "[33,    20] loss: 0.01436\n",
      "[33,    40] loss: 0.01434\n",
      "[33,    60] loss: 0.01445\n",
      "[33,    80] loss: 0.01438\n",
      "[33,   100] loss: 0.01432\n",
      "[33,   120] loss: 0.01426\n",
      "accuracy: 0.459\n",
      "[34,    20] loss: 0.01421\n",
      "[34,    40] loss: 0.01419\n",
      "[34,    60] loss: 0.01421\n",
      "[34,    80] loss: 0.01425\n",
      "[34,   100] loss: 0.01417\n",
      "[34,   120] loss: 0.01426\n",
      "accuracy: 0.466\n",
      "[35,    20] loss: 0.01400\n",
      "[35,    40] loss: 0.01411\n",
      "[35,    60] loss: 0.01418\n",
      "[35,    80] loss: 0.01421\n",
      "[35,   100] loss: 0.01405\n",
      "[35,   120] loss: 0.01412\n",
      "accuracy: 0.475\n",
      "[36,    20] loss: 0.01401\n",
      "[36,    40] loss: 0.01383\n",
      "[36,    60] loss: 0.01409\n",
      "[36,    80] loss: 0.01405\n",
      "[36,   100] loss: 0.01399\n",
      "[36,   120] loss: 0.01389\n",
      "accuracy: 0.468\n",
      "[37,    20] loss: 0.01393\n",
      "[37,    40] loss: 0.01390\n",
      "[37,    60] loss: 0.01391\n",
      "[37,    80] loss: 0.01387\n",
      "[37,   100] loss: 0.01385\n",
      "[37,   120] loss: 0.01385\n",
      "accuracy: 0.477\n",
      "[38,    20] loss: 0.01377\n",
      "[38,    40] loss: 0.01369\n",
      "[38,    60] loss: 0.01362\n",
      "[38,    80] loss: 0.01385\n",
      "[38,   100] loss: 0.01382\n",
      "[38,   120] loss: 0.01382\n",
      "accuracy: 0.482\n",
      "[39,    20] loss: 0.01375\n",
      "[39,    40] loss: 0.01374\n",
      "[39,    60] loss: 0.01355\n",
      "[39,    80] loss: 0.01367\n",
      "[39,   100] loss: 0.01373\n",
      "[39,   120] loss: 0.01372\n",
      "accuracy: 0.480\n",
      "[40,    20] loss: 0.01361\n",
      "[40,    40] loss: 0.01358\n",
      "[40,    60] loss: 0.01363\n",
      "[40,    80] loss: 0.01359\n",
      "[40,   100] loss: 0.01373\n",
      "[40,   120] loss: 0.01364\n",
      "accuracy: 0.487\n",
      "[41,    20] loss: 0.01357\n",
      "[41,    40] loss: 0.01347\n",
      "[41,    60] loss: 0.01347\n",
      "[41,    80] loss: 0.01355\n",
      "[41,   100] loss: 0.01353\n",
      "[41,   120] loss: 0.01359\n",
      "accuracy: 0.491\n",
      "[42,    20] loss: 0.01349\n",
      "[42,    40] loss: 0.01366\n",
      "[42,    60] loss: 0.01354\n",
      "[42,    80] loss: 0.01351\n",
      "[42,   100] loss: 0.01342\n",
      "[42,   120] loss: 0.01357\n",
      "accuracy: 0.489\n",
      "[43,    20] loss: 0.01359\n",
      "[43,    40] loss: 0.01342\n",
      "[43,    60] loss: 0.01344\n",
      "[43,    80] loss: 0.01346\n",
      "[43,   100] loss: 0.01344\n",
      "[43,   120] loss: 0.01337\n",
      "accuracy: 0.493\n",
      "[44,    20] loss: 0.01329\n",
      "[44,    40] loss: 0.01337\n",
      "[44,    60] loss: 0.01344\n",
      "[44,    80] loss: 0.01335\n",
      "[44,   100] loss: 0.01332\n",
      "[44,   120] loss: 0.01341\n",
      "accuracy: 0.491\n",
      "[45,    20] loss: 0.01332\n",
      "[45,    40] loss: 0.01329\n",
      "[45,    60] loss: 0.01328\n",
      "[45,    80] loss: 0.01326\n",
      "[45,   100] loss: 0.01325\n",
      "[45,   120] loss: 0.01336\n",
      "accuracy: 0.491\n",
      "[46,    20] loss: 0.01323\n",
      "[46,    40] loss: 0.01322\n",
      "[46,    60] loss: 0.01321\n",
      "[46,    80] loss: 0.01315\n",
      "[46,   100] loss: 0.01334\n",
      "[46,   120] loss: 0.01332\n",
      "accuracy: 0.499\n",
      "[47,    20] loss: 0.01324\n",
      "[47,    40] loss: 0.01326\n",
      "[47,    60] loss: 0.01307\n",
      "[47,    80] loss: 0.01318\n",
      "[47,   100] loss: 0.01315\n",
      "[47,   120] loss: 0.01302\n",
      "accuracy: 0.497\n",
      "[48,    20] loss: 0.01314\n",
      "[48,    40] loss: 0.01318\n",
      "[48,    60] loss: 0.01328\n",
      "[48,    80] loss: 0.01319\n",
      "[48,   100] loss: 0.01310\n",
      "[48,   120] loss: 0.01312\n",
      "accuracy: 0.505\n",
      "[49,    20] loss: 0.01301\n",
      "[49,    40] loss: 0.01308\n",
      "[49,    60] loss: 0.01319\n",
      "[49,    80] loss: 0.01307\n",
      "[49,   100] loss: 0.01303\n",
      "[49,   120] loss: 0.01295\n",
      "accuracy: 0.500\n",
      "[50,    20] loss: 0.01304\n",
      "[50,    40] loss: 0.01313\n",
      "[50,    60] loss: 0.01300\n",
      "[50,    80] loss: 0.01305\n",
      "[50,   100] loss: 0.01299\n",
      "[50,   120] loss: 0.01304\n",
      "accuracy: 0.507\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "from impl.transformer_modules import DecoderTransformer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = DecoderTransformer(config)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "accuracy_list = []\n",
    "\n",
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "    accuracy = 0\n",
    "    total = 0\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(nums_dl, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(\n",
    "            rearrange(outputs, 'batch seq vocab -> batch vocab seq'),\n",
    "            labels\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 20 == 19:    # print every 20 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.5f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "        \n",
    "        \n",
    "    for (x, y) in nums_tl:\n",
    "\n",
    "        x = x.to(config.device)\n",
    "        y = y.to(config.device)\n",
    "\n",
    "        y_hat = model(x)\n",
    "        y_predictions = y_hat.argmax(2)\n",
    "        accuracy += (y_predictions == y).sum().item()\n",
    "        total += y.size(0) * 6\n",
    "\n",
    "        accuracy_list.append(accuracy/total)\n",
    "    print(f'accuracy: {accuracy/total:.3f}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 6])\n",
      "torch.Size([64, 6])\n",
      "torch.Size([64, 6, 10])\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape)\n",
    "print(labels.shape)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 6, 10])\n",
      "torch.Size([64, 10, 6])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.shape)\n",
    "print(outputs.transpose(1,2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 3, 2, 5, 7, 9]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[8, 4, 9, 2, 3, 1]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = t.randint(1,10, (1, 6))\n",
    "print(arr)\n",
    "model(arr).argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 9, 8, 5, 5, 6])\n",
      "tensor([6, 5, 5, 8, 9, 7])\n"
     ]
    }
   ],
   "source": [
    "for x, y in nums_dl:\n",
    "    print(x[0])\n",
    "    print(y[0])\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a41fdc720b403cff5d22ec3440153970555b5fcc336583b0458a17a41b31d53f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
